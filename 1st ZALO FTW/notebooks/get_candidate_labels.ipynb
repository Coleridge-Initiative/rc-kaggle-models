{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.039635,
     "end_time": "2021-06-26T01:05:40.400292",
     "exception": false,
     "start_time": "2021-06-26T01:05:40.360657",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### The Purpose of This Notebook\n",
    "\n",
    "The issue with this competition is that there are a lot of dataset labels that aren't provided on purpose. Because the missing labels cause **True Positive** samples to be regarded **True Negative**, training the model on this data creates uncertainty for the model. To address this problem, we extract candidate labels in two ways, as shown below, and use them in the train/validation process:\n",
    "\n",
    "    1. Use AbbreviationDetector from scispacy to detect all capitalize string in the form \"LONG-NAME (ACRONYM)\" then only use the labels that contain keywords like (Dataset, Database, Study, Survey, ...).\n",
    "    2. (Optional) We detect the keywords (Dataset, Database, Study, Survey, ...) position in the input string then look forward/backward of that keyword util meet two consecutive lowercase words.\n",
    "\n",
    "For all found labels from the above steps, only labels that have **Jaccard Similarity** with any original train labels will be used for training, the rest will be passed to validation. \n",
    "\n",
    "**Note**: The strategy here is that, we try to found as many candidate labels as posible and let the model to learn if they are a dataset title or not rather than consider them as **True Negative** (caused by label missing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCISPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:12:09.976404Z",
     "iopub.status.busy": "2021-06-26T01:12:09.970616Z",
     "iopub.status.idle": "2021-06-26T01:12:20.469172Z",
     "shell.execute_reply": "2021-06-26T01:12:20.468668Z",
     "shell.execute_reply.started": "2021-06-26T00:46:46.331904Z"
    },
    "papermill": {
     "duration": 10.55754,
     "end_time": "2021-06-26T01:12:20.469308",
     "exception": false,
     "start_time": "2021-06-26T01:12:09.911768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Optional, Set, Dict\n",
    "from collections import defaultdict\n",
    "from spacy.tokens import Span, Doc\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.language import Language\n",
    "\n",
    "import spacy\n",
    "import pickle\n",
    "\n",
    "\n",
    "def find_abbreviation(\n",
    "    long_form_candidate: Span, short_form_candidate: Span\n",
    ") -> Tuple[Span, Optional[Span]]:\n",
    "    \"\"\"\n",
    "    Implements the abbreviation detection algorithm in \"A simple algorithm\n",
    "    for identifying abbreviation definitions in biomedical text.\", (Schwartz & Hearst, 2003).\n",
    "    The algorithm works by enumerating the characters in the short form of the abbreviation,\n",
    "    checking that they can be matched against characters in a candidate text for the long form\n",
    "    in order, as well as requiring that the first letter of the abbreviated form matches the\n",
    "    _beginning_ letter of a word.\n",
    "    Parameters\n",
    "    ----------\n",
    "    long_form_candidate: Span, required.\n",
    "        The spaCy span for the long form candidate of the definition.\n",
    "    short_form_candidate: Span, required.\n",
    "        The spaCy span for the abbreviation candidate.\n",
    "    Returns\n",
    "    -------\n",
    "    A Tuple[Span, Optional[Span]], representing the short form abbreviation and the\n",
    "    span corresponding to the long form expansion, or None if a match is not found.\n",
    "    \"\"\"\n",
    "    long_form = \" \".join([x.text for x in long_form_candidate])\n",
    "    short_form = \" \".join([x.text for x in short_form_candidate])\n",
    "\n",
    "    long_index = len(long_form) - 1\n",
    "    short_index = len(short_form) - 1\n",
    "\n",
    "    while short_index >= 0:\n",
    "        current_char = short_form[short_index].lower()\n",
    "        # We don't check non alpha-numeric characters.\n",
    "        if not current_char.isalnum():\n",
    "            short_index -= 1\n",
    "            continue\n",
    "\n",
    "            # Does the character match at this position? ...\n",
    "        while (\n",
    "            (long_index >= 0 and long_form[long_index].lower() != current_char)\n",
    "            or\n",
    "            # .... or if we are checking the first character of the abbreviation, we enforce\n",
    "            # to be the _starting_ character of a span.\n",
    "            (\n",
    "                short_index == 0\n",
    "                and long_index > 0\n",
    "                and long_form[long_index - 1].isalnum()\n",
    "            )\n",
    "        ):\n",
    "            long_index -= 1\n",
    "\n",
    "        if long_index < 0:\n",
    "            return short_form_candidate, None\n",
    "\n",
    "        long_index -= 1\n",
    "        short_index -= 1\n",
    "\n",
    "    # The last subtraction will either take us on to a whitespace character, or\n",
    "    # off the front of the string (i.e. long_index == -1). Either way, we want to add\n",
    "    # one to get back to the start character of the long form\n",
    "    long_index += 1\n",
    "\n",
    "    # Now we know the character index of the start of the character span,\n",
    "    # here we just translate that to the first token beginning after that\n",
    "    # value, so we can return a spaCy span instead.\n",
    "    word_lengths = 0\n",
    "    starting_index = None\n",
    "    for i, word in enumerate(long_form_candidate):\n",
    "        # need to add 1 for the space characters\n",
    "        word_lengths += len(word.text_with_ws)\n",
    "        if word_lengths > long_index:\n",
    "            starting_index = i\n",
    "            break\n",
    "\n",
    "    return short_form_candidate, long_form_candidate[starting_index:]\n",
    "\n",
    "\n",
    "def filter_matches(\n",
    "    matcher_output: List[Tuple[int, int, int]], doc: Doc\n",
    ") -> List[Tuple[Span, Span]]:\n",
    "    # Filter into two cases:\n",
    "    # 1. <Short Form> ( <Long Form> )\n",
    "    # 2. <Long Form> (<Short Form>) [this case is most common].\n",
    "    candidates = []\n",
    "    for match in matcher_output:\n",
    "        start = match[1]\n",
    "        end = match[2]\n",
    "        # Ignore spans with more than 8 words in them, and spans at the start of the doc\n",
    "        if end - start > 8 or start == 1:\n",
    "            continue\n",
    "        if end - start > 3:\n",
    "            # Long form is inside the parens.\n",
    "            # Take one word before.\n",
    "            short_form_candidate = doc[start - 2 : start - 1]\n",
    "            long_form_candidate = doc[start:end]\n",
    "        else:\n",
    "            # Normal case.\n",
    "            # Short form is inside the parens.\n",
    "            short_form_candidate = doc[start:end]\n",
    "\n",
    "            # Sum character lengths of contents of parens.\n",
    "            abbreviation_length = sum([len(x) for x in short_form_candidate])\n",
    "            max_words = min(abbreviation_length + 5, abbreviation_length * 2)\n",
    "            # Look up to max_words backwards\n",
    "            long_form_candidate = doc[max(start - max_words - 1, 0) : start - 1]\n",
    "\n",
    "        # add candidate to candidates if candidates pass filters\n",
    "        if short_form_filter(short_form_candidate):\n",
    "            candidates.append((long_form_candidate, short_form_candidate))\n",
    "\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def short_form_filter(span: Span) -> bool:\n",
    "    # All words are between length 2 and 10\n",
    "    if not all([2 <= len(x) < 10 for x in span]):\n",
    "        return False\n",
    "\n",
    "    # At least 50% of the short form should be alpha\n",
    "    if (sum([c.isalpha() for c in span.text]) / len(span.text)) < 0.5:\n",
    "        return False\n",
    "\n",
    "    # The first character of the short form should be alpha\n",
    "    if not span.text[0].isalpha():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "@Language.factory(\"abbreviation_detector\")\n",
    "class AbbreviationDetector:\n",
    "    \"\"\"\n",
    "    Detects abbreviations using the algorithm in \"A simple algorithm for identifying\n",
    "    abbreviation definitions in biomedical text.\", (Schwartz & Hearst, 2003).\n",
    "    This class sets the `._.abbreviations` attribute on spaCy Doc.\n",
    "    The abbreviations attribute is a `List[Span]` where each Span has the `Span._.long_form`\n",
    "    attribute set to the long form definition of the abbreviation.\n",
    "    Note that this class does not replace the spans, or merge them.\n",
    "    Parameters\n",
    "    ----------\n",
    "    nlp: `Language`, a required argument for spacy to use this as a factory\n",
    "    name: `str`, a required argument for spacy to use this as a factory\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nlp: Language, name: str = \"abbreviation_detector\") -> None:\n",
    "        Doc.set_extension(\"abbreviations\", default=[], force=True)\n",
    "        Span.set_extension(\"long_form\", default=None, force=True)\n",
    "\n",
    "        self.matcher = Matcher(nlp.vocab)\n",
    "        self.matcher.add(\"parenthesis\", [[{\"ORTH\": \"(\"}, {\"OP\": \"+\"}, {\"ORTH\": \")\"}]])\n",
    "        self.global_matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    def find(self, span: Span, doc: Doc) -> Tuple[Span, Set[Span]]:\n",
    "        \"\"\"\n",
    "        Functional version of calling the matcher for a single span.\n",
    "        This method is helpful if you already have an abbreviation which\n",
    "        you want to find a definition for.\n",
    "        \"\"\"\n",
    "        dummy_matches = [(-1, int(span.start), int(span.end))]\n",
    "        filtered = filter_matches(dummy_matches, doc)\n",
    "        abbreviations = self.find_matches_for(filtered, doc)\n",
    "\n",
    "        if not abbreviations:\n",
    "            return span, set()\n",
    "        else:\n",
    "            return abbreviations[0]\n",
    "\n",
    "    def __call__(self, doc: Doc) -> Doc:\n",
    "        matches = self.matcher(doc)\n",
    "        matches_no_brackets = [(x[0], x[1] + 1, x[2] - 1) for x in matches]\n",
    "        filtered = filter_matches(matches_no_brackets, doc)\n",
    "        occurences = self.find_matches_for(filtered, doc)\n",
    "\n",
    "        for (long_form, short_forms) in occurences:\n",
    "            for short in short_forms:\n",
    "                short._.long_form = long_form\n",
    "                doc._.abbreviations.append(short)\n",
    "        return doc\n",
    "\n",
    "    def find_matches_for(\n",
    "        self, filtered: List[Tuple[Span, Span]], doc: Doc\n",
    "    ) -> List[Tuple[Span, Set[Span]]]:\n",
    "        rules = {}\n",
    "        all_occurences: Dict[Span, Set[Span]] = defaultdict(set)\n",
    "        already_seen_long: Set[str] = set()\n",
    "        already_seen_short: Set[str] = set()\n",
    "        for (long_candidate, short_candidate) in filtered:\n",
    "            short, long = find_abbreviation(long_candidate, short_candidate)\n",
    "            # We need the long and short form definitions to be unique, because we need\n",
    "            # to store them so we can look them up later. This is a bit of a\n",
    "            # pathalogical case also, as it would mean an abbreviation had been\n",
    "            # defined twice in a document. There's not much we can do about this,\n",
    "            # but at least the case which is discarded will be picked up below by\n",
    "            # the global matcher. So it's likely that things will work out ok most of the time.\n",
    "            new_long = long.text not in already_seen_long if long else False\n",
    "            new_short = short.text not in already_seen_short\n",
    "            if long is not None and new_long and new_short:\n",
    "                already_seen_long.add(long.text)\n",
    "                already_seen_short.add(short.text)\n",
    "                all_occurences[long].add(short)\n",
    "                rules[long.text] = long\n",
    "                # Add a rule to a matcher to find exactly this substring.\n",
    "                self.global_matcher.add(long.text, [[{\"ORTH\": x.text} for x in short]])\n",
    "        to_remove = set()\n",
    "        global_matches = self.global_matcher(doc)\n",
    "        for match, start, end in global_matches:\n",
    "            string_key = self.global_matcher.vocab.strings[match]\n",
    "            to_remove.add(string_key)\n",
    "            all_occurences[rules[string_key]].add(doc[start:end])\n",
    "        for key in to_remove:\n",
    "            # Clean up the global matcher.\n",
    "            self.global_matcher.remove(key)\n",
    "\n",
    "        return list((k, v) for k, v in all_occurences.items())\n",
    "    \n",
    "    \n",
    "def get_acronym(doc):\n",
    "    doc = nlp(doc)\n",
    "    short_form = []\n",
    "    long_form = []\n",
    "    for abrv in doc._.abbreviations:\n",
    "        short_form.append(abrv)\n",
    "        long_form.append(abrv._.long_form.text)\n",
    "    return short_form, long_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ../data/en_core_sci_lg-0.4.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:12:20.579047Z",
     "iopub.status.busy": "2021-06-26T01:12:20.578562Z",
     "iopub.status.idle": "2021-06-26T01:12:36.909542Z",
     "shell.execute_reply": "2021-06-26T01:12:36.909095Z",
     "shell.execute_reply.started": "2021-06-26T00:46:56.116033Z"
    },
    "papermill": {
     "duration": 16.389453,
     "end_time": "2021-06-26T01:12:36.909660",
     "exception": false,
     "start_time": "2021-06-26T01:12:20.520207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_sci_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:12:37.021593Z",
     "iopub.status.busy": "2021-06-26T01:12:37.019437Z",
     "iopub.status.idle": "2021-06-26T01:12:37.026088Z",
     "shell.execute_reply": "2021-06-26T01:12:37.025522Z",
     "shell.execute_reply.started": "2021-06-26T00:47:16.912072Z"
    },
    "papermill": {
     "duration": 0.066919,
     "end_time": "2021-06-26T01:12:37.026253",
     "exception": false,
     "start_time": "2021-06-26T01:12:36.959334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp.add_pipe(\"abbreviation_detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.077252,
     "end_time": "2021-06-26T01:12:37.347349",
     "exception": false,
     "start_time": "2021-06-26T01:12:37.270097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:12:37.690159Z",
     "iopub.status.busy": "2021-06-26T01:12:37.689510Z",
     "iopub.status.idle": "2021-06-26T01:12:39.991593Z",
     "shell.execute_reply": "2021-06-26T01:12:39.991157Z",
     "shell.execute_reply.started": "2021-06-26T00:47:16.963831Z"
    },
    "papermill": {
     "duration": 2.390033,
     "end_time": "2021-06-26T01:12:39.991704",
     "exception": false,
     "start_time": "2021-06-26T01:12:37.601671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from collections import Counter\n",
    "import glob\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = json.load(open(\"../settings.json\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in settings.items():\n",
    "    settings[k] = \".\" + v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:12:40.104614Z",
     "iopub.status.busy": "2021-06-26T01:12:40.103605Z",
     "iopub.status.idle": "2021-06-26T01:12:40.108468Z",
     "shell.execute_reply": "2021-06-26T01:12:40.107874Z",
     "shell.execute_reply.started": "2021-06-26T00:47:18.983088Z"
    },
    "papermill": {
     "duration": 0.064932,
     "end_time": "2021-06-26T01:12:40.108585",
     "exception": false,
     "start_time": "2021-06-26T01:12:40.043653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_s_e_window_sliding(sample_len, win_size, step_size):\n",
    "    start = 0\n",
    "    end = win_size\n",
    "    s_e = []\n",
    "    s_e.append([start, end])\n",
    "    while end < sample_len:\n",
    "        start += step_size\n",
    "        end = start + win_size\n",
    "        s_e.append([start, end])\n",
    "\n",
    "    s_e[-1][0] -= s_e[-1][1] - sample_len\n",
    "    s_e[-1][0] = max(s_e[-1][0], 0)\n",
    "    s_e[-1][1] = sample_len\n",
    "    return s_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:12:40.219016Z",
     "iopub.status.busy": "2021-06-26T01:12:40.218372Z",
     "iopub.status.idle": "2021-06-26T01:12:40.367381Z",
     "shell.execute_reply": "2021-06-26T01:12:40.366694Z",
     "shell.execute_reply.started": "2021-06-26T00:47:18.991281Z"
    },
    "papermill": {
     "duration": 0.206124,
     "end_time": "2021-06-26T01:12:40.367531",
     "exception": false,
     "start_time": "2021-06-26T01:12:40.161407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(settings[\"RAW_DATA_DIR\"], \"train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:12:40.539370Z",
     "iopub.status.busy": "2021-06-26T01:12:40.538847Z",
     "iopub.status.idle": "2021-06-26T01:12:40.546485Z",
     "shell.execute_reply": "2021-06-26T01:12:40.545949Z",
     "shell.execute_reply.started": "2021-06-26T00:47:19.146916Z"
    },
    "papermill": {
     "duration": 0.099248,
     "end_time": "2021-06-26T01:12:40.546599",
     "exception": false,
     "start_time": "2021-06-26T01:12:40.447351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_label = train_df.cleaned_label.tolist()\n",
    "dataset_label = train_df.cleaned_label.tolist()\n",
    "dataset_title = train_df.dataset_title.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:12:40.664048Z",
     "iopub.status.busy": "2021-06-26T01:12:40.663396Z",
     "iopub.status.idle": "2021-06-26T01:12:40.677954Z",
     "shell.execute_reply": "2021-06-26T01:12:40.677362Z",
     "shell.execute_reply.started": "2021-06-26T00:47:19.164124Z"
    },
    "papermill": {
     "duration": 0.08073,
     "end_time": "2021-06-26T01:12:40.678111",
     "exception": false,
     "start_time": "2021-06-26T01:12:40.597381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_1 = [x.lower().strip() for x in train_df['dataset_label'].unique()]\n",
    "temp_2 = [x.lower().strip() for x in train_df['dataset_title'].unique()]\n",
    "temp_3 = [x.lower().strip() for x in train_df['cleaned_label'].unique()]\n",
    "all_train_labels = list(set(temp_1 + temp_2 + temp_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.053661,
     "end_time": "2021-06-26T01:12:40.797343",
     "exception": false,
     "start_time": "2021-06-26T01:12:40.743682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:12:40.907827Z",
     "iopub.status.busy": "2021-06-26T01:12:40.907158Z",
     "iopub.status.idle": "2021-06-26T01:12:41.223378Z",
     "shell.execute_reply": "2021-06-26T01:12:41.222793Z",
     "shell.execute_reply.started": "2021-06-26T00:47:19.187734Z"
    },
    "papermill": {
     "duration": 0.373786,
     "end_time": "2021-06-26T01:12:41.223522",
     "exception": false,
     "start_time": "2021-06-26T01:12:40.849736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_IDS = glob.glob(os.path.join(settings[\"RAW_DATA_DIR\"], \"train/*\"))\n",
    "TRAIN_IDS = [TRAIN_ID.split(\"/\")[-1].split(\".\")[0] for TRAIN_ID in TRAIN_IDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(TRAIN_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:12:41.396218Z",
     "iopub.status.busy": "2021-06-26T01:12:41.395540Z",
     "iopub.status.idle": "2021-06-26T01:13:03.901919Z",
     "shell.execute_reply": "2021-06-26T01:13:03.901214Z",
     "shell.execute_reply.started": "2021-06-26T00:47:19.640589Z"
    },
    "papermill": {
     "duration": 22.597864,
     "end_time": "2021-06-26T01:13:03.902058",
     "exception": false,
     "start_time": "2021-06-26T01:12:41.304194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "win_size = 30\n",
    "\n",
    "def process(i):\n",
    "    ids = []\n",
    "    texts = []\n",
    "    labels = []\n",
    "    pub_titles = []\n",
    "    cleaned_labels = []\n",
    "    x = json.load(open(\n",
    "        f\"{settings['RAW_DATA_DIR']}/train/{TRAIN_IDS[i]}.json\",\"rt\"))\n",
    "    label = \"unknow\"\n",
    "    full_text = \"\"\n",
    "    unique_id = []\n",
    "    for section in x:\n",
    "        raw_text = section[\"text\"].replace(\"\\n\", \" \")\n",
    "        raw_text_encode = raw_text.split()\n",
    "        s_e = generate_s_e_window_sliding(len(raw_text_encode), win_size, int(0.5 * win_size))\n",
    "        for (s, e) in s_e:\n",
    "            sent = \" \".join(raw_text_encode[s:e]).strip()\n",
    "            texts.append(sent)\n",
    "            ids.append(TRAIN_IDS[i])\n",
    "            labels.append(label)\n",
    "        full_text += section[\"text\"].replace(\"\\n\", \" \") + \" \"\n",
    "    \n",
    "    unique_id = TRAIN_IDS[i]\n",
    "    full_text = full_text.strip()\n",
    "\n",
    "    results = {}\n",
    "    results[\"id\"] = ids\n",
    "    results[\"text\"] = texts\n",
    "    results[\"label\"] = labels\n",
    "    results[\"unique_id\"] = unique_id\n",
    "    results[\"full_text\"] = full_text\n",
    "    return results\n",
    "        \n",
    "# define map iterator\n",
    "def iterator_data(items_list):\n",
    "    for item in items_list:\n",
    "        yield item\n",
    "\n",
    "iterator_data = iterator_data(range(len(TRAIN_IDS)))\n",
    "p = Pool(8)\n",
    "\n",
    "partial_fn = partial(process)\n",
    "train_map = p.imap(\n",
    "    partial_fn,\n",
    "    tqdm(iterator_data, total=len(TRAIN_IDS), desc=\"[Preprocessing TestSet]\"),\n",
    "    chunksize=10,\n",
    ")\n",
    "\n",
    "results = []\n",
    "for result in tqdm(train_map):\n",
    "    results.append(result)\n",
    "\n",
    "ids = []\n",
    "texts = []\n",
    "labels = []\n",
    "unique_ids = []\n",
    "full_texts = []\n",
    "for result in tqdm(results):\n",
    "    ids.extend(result[\"id\"])\n",
    "    texts.extend(result[\"text\"])\n",
    "    labels.extend(result[\"label\"])\n",
    "    unique_ids.append(result[\"unique_id\"])\n",
    "    full_texts.append(result[\"full_text\"])\n",
    "    \n",
    "test_df = pd.DataFrame()\n",
    "test_df[\"id\"] = ids\n",
    "test_df[\"text\"] = texts\n",
    "test_df[\"label\"] = labels\n",
    "test_df[\"group\"] = [-1] * len(ids)\n",
    "test_df[\"title\"] = [\"\"] * len(ids)\n",
    "\n",
    "p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:13:04.973039Z",
     "iopub.status.busy": "2021-06-26T01:13:04.972207Z",
     "iopub.status.idle": "2021-06-26T01:13:04.974447Z",
     "shell.execute_reply": "2021-06-26T01:13:04.975050Z",
     "shell.execute_reply.started": "2021-06-26T00:47:49.485409Z"
    },
    "papermill": {
     "duration": 0.803791,
     "end_time": "2021-06-26T01:13:04.975218",
     "exception": false,
     "start_time": "2021-06-26T01:13:04.171427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_texts = test_df[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.239537,
     "end_time": "2021-06-26T01:13:05.454876",
     "exception": false,
     "start_time": "2021-06-26T01:13:05.215339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:13:05.939820Z",
     "iopub.status.busy": "2021-06-26T01:13:05.938993Z",
     "iopub.status.idle": "2021-06-26T01:13:05.944359Z",
     "shell.execute_reply": "2021-06-26T01:13:05.944845Z",
     "shell.execute_reply.started": "2021-06-26T00:47:49.694771Z"
    },
    "papermill": {
     "duration": 0.249364,
     "end_time": "2021-06-26T01:13:05.945022",
     "exception": false,
     "start_time": "2021-06-26T01:13:05.695658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_valid_text(string):\n",
    "    \"\"\"\n",
    "    Check if the input string contains \n",
    "    below accepted_keywords or not\n",
    "    \"\"\"\n",
    "    accepted_keywords = [\"Study\", \"Studies\", \"Survey\", \n",
    "                         \"Surveys\", \"Dataset\", \"Datasets\", \n",
    "                         \"Database\", \"Databases\", \"Data Set\", \n",
    "                         \"Data System\", \"Data Systems\"]\n",
    "#                          \"Program\", \"Programs\", \"Programme\"]\n",
    "    for k in accepted_keywords:\n",
    "        if k in string:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:13:06.431103Z",
     "iopub.status.busy": "2021-06-26T01:13:06.430280Z",
     "iopub.status.idle": "2021-06-26T01:13:27.947751Z",
     "shell.execute_reply": "2021-06-26T01:13:27.947201Z",
     "shell.execute_reply.started": "2021-06-26T00:47:49.748632Z"
    },
    "papermill": {
     "duration": 21.761566,
     "end_time": "2021-06-26T01:13:27.947865",
     "exception": false,
     "start_time": "2021-06-26T01:13:06.186299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_texts = []\n",
    "\n",
    "for text in tqdm(all_texts):\n",
    "    if check_valid_text(text):\n",
    "        valid_texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:13:28.374135Z",
     "iopub.status.busy": "2021-06-26T01:13:28.373522Z",
     "iopub.status.idle": "2021-06-26T01:13:28.376513Z",
     "shell.execute_reply": "2021-06-26T01:13:28.376880Z",
     "shell.execute_reply.started": "2021-06-26T00:48:18.726631Z"
    },
    "papermill": {
     "duration": 0.217322,
     "end_time": "2021-06-26T01:13:28.377031",
     "exception": false,
     "start_time": "2021-06-26T01:13:28.159709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(valid_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:13:28.841198Z",
     "iopub.status.busy": "2021-06-26T01:13:28.840704Z",
     "iopub.status.idle": "2021-06-26T01:13:28.846467Z",
     "shell.execute_reply": "2021-06-26T01:13:28.845873Z",
     "shell.execute_reply.started": "2021-06-26T00:48:18.735227Z"
    },
    "papermill": {
     "duration": 0.260623,
     "end_time": "2021-06-26T01:13:28.846578",
     "exception": false,
     "start_time": "2021-06-26T01:13:28.585955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_text = \" \".join(valid_texts)  # concatenate all valid strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:13:29.281026Z",
     "iopub.status.busy": "2021-06-26T01:13:29.280425Z",
     "iopub.status.idle": "2021-06-26T01:13:29.283900Z",
     "shell.execute_reply": "2021-06-26T01:13:29.283482Z",
     "shell.execute_reply.started": "2021-06-26T00:48:18.811959Z"
    },
    "papermill": {
     "duration": 0.229268,
     "end_time": "2021-06-26T01:13:29.284013",
     "exception": false,
     "start_time": "2021-06-26T01:13:29.054745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split full_text into chunks (charactor length is 100000)\n",
    "chunk_texts = []\n",
    "len_chunk = 100000\n",
    "\n",
    "for start in range(0, len(full_text), len_chunk):\n",
    "    chunk_texts.append(full_text[start:start + len_chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:13:29.706933Z",
     "iopub.status.busy": "2021-06-26T01:13:29.706393Z",
     "iopub.status.idle": "2021-06-26T01:13:29.710191Z",
     "shell.execute_reply": "2021-06-26T01:13:29.710605Z",
     "shell.execute_reply.started": "2021-06-26T00:48:18.838766Z"
    },
    "papermill": {
     "duration": 0.21629,
     "end_time": "2021-06-26T01:13:29.710762",
     "exception": false,
     "start_time": "2021-06-26T01:13:29.494472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:13:30.137742Z",
     "iopub.status.busy": "2021-06-26T01:13:30.137152Z",
     "iopub.status.idle": "2021-06-26T01:13:30.139930Z",
     "shell.execute_reply": "2021-06-26T01:13:30.140326Z",
     "shell.execute_reply.started": "2021-06-26T00:48:18.848620Z"
    },
    "papermill": {
     "duration": 0.21955,
     "end_time": "2021-06-26T01:13:30.140472",
     "exception": false,
     "start_time": "2021-06-26T01:13:29.920922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "accepted_preds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:13:30.562980Z",
     "iopub.status.busy": "2021-06-26T01:13:30.562477Z",
     "iopub.status.idle": "2021-06-26T01:19:16.470263Z",
     "shell.execute_reply": "2021-06-26T01:19:16.469720Z",
     "shell.execute_reply.started": "2021-06-26T00:48:18.863368Z"
    },
    "papermill": {
     "duration": 346.122201,
     "end_time": "2021-06-26T01:19:16.470385",
     "exception": false,
     "start_time": "2021-06-26T01:13:30.348184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Multiprocess finding all **LONG FORM (SHORT FORM)** \n",
    "\n",
    "def iterator_data(items_list):\n",
    "    for item in items_list:\n",
    "        yield item\n",
    "\n",
    "def get_extra_label(string):\n",
    "    short_form, long_form = get_acronym(string)\n",
    "    return long_form\n",
    "\n",
    "\n",
    "partial_fn = partial(get_extra_label)\n",
    "extra_labels = []\n",
    "\n",
    "for i in range(0, len(chunk_texts), 48):\n",
    "    p = Pool(8)\n",
    "    extra_label_results = p.imap(\n",
    "        partial_fn,\n",
    "        tqdm(iterator_data(chunk_texts[i:i+48]), total=48, desc=\"[Get extra labels]\"),\n",
    "        chunksize=10,\n",
    "    )\n",
    "\n",
    "    for result in tqdm(extra_label_results):\n",
    "        extra_labels.extend(result)\n",
    "\n",
    "    p.close()\n",
    "\n",
    "extra_labels = list(set(extra_labels))\n",
    "\n",
    "for extra_label in extra_labels:\n",
    "    if extra_label.islower() is False:\n",
    "        accepted_preds.append(extra_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:19:17.447823Z",
     "iopub.status.busy": "2021-06-26T01:19:17.446982Z",
     "iopub.status.idle": "2021-06-26T01:19:17.452957Z",
     "shell.execute_reply": "2021-06-26T01:19:17.452366Z",
     "shell.execute_reply.started": "2021-06-26T00:57:03.564126Z"
    },
    "papermill": {
     "duration": 0.359239,
     "end_time": "2021-06-26T01:19:17.453108",
     "exception": false,
     "start_time": "2021-06-26T01:19:17.093869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(accepted_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.351616,
     "end_time": "2021-06-26T01:19:18.157764",
     "exception": false,
     "start_time": "2021-06-26T01:19:17.806148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:19:18.868023Z",
     "iopub.status.busy": "2021-06-26T01:19:18.867317Z",
     "iopub.status.idle": "2021-06-26T01:19:18.871367Z",
     "shell.execute_reply": "2021-06-26T01:19:18.870619Z",
     "shell.execute_reply.started": "2021-06-26T00:57:03.576570Z"
    },
    "papermill": {
     "duration": 0.362641,
     "end_time": "2021-06-26T01:19:18.871508",
     "exception": false,
     "start_time": "2021-06-26T01:19:18.508867",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Forward/Backward Complete Dataset Finding\n",
    "\n",
    "**Note**: This is an optional step, only the labels provided by scispacy is good enough to reproduce our results but there are still many candidate labels that not match the form \"LONG FORM (SHORT FORM)\", for example only LONG FORM, so this algorithm still yield more candidate labels that could improve the performance of the model (but not much in our experiments). Our latest submission used this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:19:19.575818Z",
     "iopub.status.busy": "2021-06-26T01:19:19.575101Z",
     "iopub.status.idle": "2021-06-26T01:19:19.578198Z",
     "shell.execute_reply": "2021-06-26T01:19:19.578692Z",
     "shell.execute_reply.started": "2021-06-26T00:57:03.594805Z"
    },
    "papermill": {
     "duration": 0.355063,
     "end_time": "2021-06-26T01:19:19.578880",
     "exception": false,
     "start_time": "2021-06-26T01:19:19.223817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def jaccard_similarity(str1, str2): \n",
    "    a = set(str1.lower().split(\" \"))\n",
    "    b = set(str2.lower().split(\" \"))\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "def clean_text(txt, lower=True):\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())\n",
    "\n",
    "def check_splcharacter(test):\n",
    "    string_check= re.compile('[@_!#$%^&*()<>?/\\|}{~:.]')\n",
    " \n",
    "    if(string_check.search(test) == None):\n",
    "        return True\n",
    "    else: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:19:20.992835Z",
     "iopub.status.busy": "2021-06-26T01:19:20.992092Z",
     "iopub.status.idle": "2021-06-26T01:19:20.994562Z",
     "shell.execute_reply": "2021-06-26T01:19:20.994032Z",
     "shell.execute_reply.started": "2021-06-26T00:57:03.740692Z"
    },
    "papermill": {
     "duration": 0.356782,
     "end_time": "2021-06-26T01:19:20.994703",
     "exception": false,
     "start_time": "2021-06-26T01:19:20.637921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_find_dataset0(string):\n",
    "    keywords = [\"Database\", \"Dataset\", \"Databases\", \"Datasets\"]\n",
    "    prepositions = [\"on\", \"for\", \"of\", \"in\"]\n",
    "    start_end_list = []\n",
    "    word_list = []\n",
    "    se2idx = {}\n",
    "    words = string.split(\" \")\n",
    "    old_s = 0\n",
    "    for i, word in enumerate(words):\n",
    "        new_s = old_s\n",
    "        start_end_list.append([new_s, new_s + len(word)])\n",
    "        se2idx[f\"{new_s}-{new_s + len(word)}\"] = i\n",
    "        word_list.append(word)\n",
    "        old_s += len(word) + 1\n",
    "        \n",
    "    candidates = []\n",
    "        \n",
    "    def complete_dataset(dataset_idx, word_list, mode=\"backward\"):\n",
    "        # go backwards until meet 2 consecutive lowercase words\n",
    "        candidate_word = []\n",
    "        n_consecutive_lower = 0\n",
    "        if mode == \"backward\":\n",
    "            for i in range(dataset_idx, 0, -1):\n",
    "                word_i = word_list[i]\n",
    "                if word_i.islower() is False and word_i.replace(\"\\'\", \"\").isalpha():\n",
    "                    candidate_word.append(word_i)\n",
    "                    n_consecutive_lower = 0 # reset it to 0\n",
    "                else:\n",
    "                    # temporal add this word, will remove\n",
    "                    # if n_consecutive_lower == 2\n",
    "                    n_consecutive_lower += 1\n",
    "                    if n_consecutive_lower == 2 or word_i.lower() in [\"a\", \"an\", \"the\"]:\n",
    "                        if n_consecutive_lower == 2:\n",
    "                            candidate_word = candidate_word[:-1]\n",
    "                        break\n",
    "                    else:\n",
    "                        candidate_word.append(word_i)\n",
    "                        \n",
    "            # remove a, an, the\n",
    "            if candidate_word[-1] in [\"A\", \"An\", \"The\"]:\n",
    "                candidate_word = candidate_word[:-1]\n",
    "            return \" \".join(candidate_word[::-1])\n",
    "        else:\n",
    "            for i in range(dataset_idx, len(word_list), 1):\n",
    "                word_i = word_list[i]\n",
    "                if word_i.islower() is False and word_i.replace(\"\\'\", \"\").isalpha():\n",
    "                    candidate_word.append(word_i)\n",
    "                    n_consecutive_lower = 0 # reset it to 0\n",
    "                else:\n",
    "                    # temporal add this word, will remove\n",
    "                    # if n_consecutive_lower == 2\n",
    "                    n_consecutive_lower += 1\n",
    "                    if n_consecutive_lower == 2 or word_i.lower() in [\"a\", \"an\", \"the\"]:\n",
    "                        if n_consecutive_lower == 2:\n",
    "                            candidate_word = candidate_word[:-1]\n",
    "                        break\n",
    "                    else:\n",
    "                        candidate_word.append(word_i)\n",
    "\n",
    "            return \" \".join(candidate_word)\n",
    "            \n",
    "    # forward, backward complete (keyword + prepositions)\n",
    "    for k in keywords:\n",
    "        for prepos in prepositions:\n",
    "            new_k = \" \" + k + f\" {prepos} \"\n",
    "            matchs = re.finditer(new_k, string)\n",
    "            for match in matchs:\n",
    "                start_index = match.start() + 1\n",
    "                end_index = match.end() - 1\n",
    "                forward_candidate = complete_dataset(se2idx[f\"{start_index}-{start_index + len(k)}\"] ,word_list, \"forward\")\n",
    "                backward_candidate = complete_dataset(se2idx[f\"{start_index}-{start_index + len(k)}\"] ,word_list, \"backward\")\n",
    "                candidate = \" \".join(backward_candidate.split(\" \") + [prepos] + forward_candidate.split(\" \")[2:])\n",
    "                words = candidate.split()\n",
    "                words = [w for i,w in enumerate(words) if w not in prepositions or i == len(words) - 1]\n",
    "                acronym = \"\".join([w[0] for w in words])\n",
    "                if len(candidate.split(\" \")) >= 3 and len(candidate.split(\" \")) <= 10 and acronym.isupper() and len(candidate) <= 60 and check_splcharacter(candidate):\n",
    "                    candidates.append(candidate)\n",
    "    \n",
    "    # backward complete\n",
    "    for k in keywords:\n",
    "        matchs = re.finditer(\" \"+ k + \" \", string)\n",
    "        for match in matchs:\n",
    "            start_index = match.start() + 1\n",
    "            end_index = match.end() - 1\n",
    "            candidate = complete_dataset(se2idx[f\"{start_index}-{end_index}\"], word_list, \"backward\")\n",
    "            words = candidate.split()\n",
    "            words = [w for i,w in enumerate(words) if w not in prepositions or i == len(words) - 1]\n",
    "            acronym = \"\".join([w[0] for w in words])\n",
    "            if len(candidate.split(\" \")) >= 3 and len(candidate.split(\" \")) <= 10 and acronym.isupper() and len(candidate) <= 60 and check_splcharacter(candidate):\n",
    "                candidates.append(candidate)\n",
    "            \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:19:21.700892Z",
     "iopub.status.busy": "2021-06-26T01:19:21.700174Z",
     "iopub.status.idle": "2021-06-26T01:19:21.704283Z",
     "shell.execute_reply": "2021-06-26T01:19:21.703203Z",
     "shell.execute_reply.started": "2021-06-26T00:57:03.853713Z"
    },
    "papermill": {
     "duration": 0.359673,
     "end_time": "2021-06-26T01:19:21.704426",
     "exception": false,
     "start_time": "2021-06-26T01:19:21.344753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_find_dataset1(string):\n",
    "    keywords = [\"Data Set\", \"Data System\", \"Data Systems\", \"Data Sets\", \"Dataset System\", \"Dataset Systems\"]\n",
    "    start_end_list = []\n",
    "    word_list = []\n",
    "    se2idx = {}\n",
    "    words = string.split(\" \")\n",
    "    old_s = 0\n",
    "    for i, word in enumerate(words):\n",
    "        new_s = old_s\n",
    "        start_end_list.append([new_s, new_s + len(word)])\n",
    "        se2idx[f\"{new_s}-{new_s + len(word)}\"] = i\n",
    "        word_list.append(word)\n",
    "        old_s += len(word) + 1\n",
    "        \n",
    "    candidates = []\n",
    "        \n",
    "    def complete_dataset(dataset_idx, word_list):\n",
    "        # go backwards until meet 2 consecutive lowercase words\n",
    "        candidate_word = []\n",
    "        n_consecutive_lower = 0\n",
    "        for i in range(dataset_idx, 0, -1):\n",
    "            word_i = word_list[i]\n",
    "            if word_i.islower() is False and word_i.replace(\"\\'\", \"\").isalpha():\n",
    "                candidate_word.append(word_i)\n",
    "                n_consecutive_lower = 0 # reset it to 0\n",
    "            else:\n",
    "                # temporal add this word, will remove\n",
    "                # if n_consecutive_lower == 2\n",
    "                n_consecutive_lower += 1\n",
    "                if n_consecutive_lower == 2 or word_i.lower() in [\"a\", \"an\", \"the\"]:\n",
    "                    if n_consecutive_lower == 2:\n",
    "                        candidate_word = candidate_word[:-1]\n",
    "                    break\n",
    "                else:\n",
    "                    candidate_word.append(word_i)\n",
    "\n",
    "        # remove a, an, the\n",
    "        if candidate_word[-1] in [\"A\", \"An\", \"The\"]:\n",
    "            candidate_word = candidate_word[:-1]\n",
    "        return \" \".join(candidate_word[::-1])\n",
    "            \n",
    "    \n",
    "    # backward complete\n",
    "    for k in keywords:\n",
    "        matchs = re.finditer(\" \"+ k + \" \", string)\n",
    "        for match in matchs:\n",
    "            start_index = match.start() + 1\n",
    "            end_index = match.end() - 1\n",
    "            candidate = complete_dataset(se2idx[f\"{start_index}-{start_index + len(k.split(' ')[0])}\"], word_list)\n",
    "            candidate += \" \" + k.split(\" \")[1]\n",
    "            words = candidate.split()\n",
    "            acronym = \"\".join([w[0] for w in words])\n",
    "            if len(candidate.split(\" \")) >= 4 and len(candidate.split(\" \")) <= 10 and acronym.isupper() and len(candidate) <= 60 and check_splcharacter(candidate):\n",
    "                candidates.append(candidate)\n",
    "\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:19:22.410058Z",
     "iopub.status.busy": "2021-06-26T01:19:22.409107Z",
     "iopub.status.idle": "2021-06-26T01:19:22.411571Z",
     "shell.execute_reply": "2021-06-26T01:19:22.410933Z",
     "shell.execute_reply.started": "2021-06-26T00:57:03.900946Z"
    },
    "papermill": {
     "duration": 0.357396,
     "end_time": "2021-06-26T01:19:22.411713",
     "exception": false,
     "start_time": "2021-06-26T01:19:22.054317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_find_dataset2(string):\n",
    "    keywords = [\"Survey\", \"Surveys\", \"Study\", \"Studies\"]\n",
    "    prepositions = [\"on\", \"for\", \"of\", \"in\"]\n",
    "    start_end_list = []\n",
    "    word_list = []\n",
    "    se2idx = {}\n",
    "    words = string.split(\" \")\n",
    "    old_s = 0\n",
    "    for i, word in enumerate(words):\n",
    "        new_s = old_s\n",
    "        start_end_list.append([new_s, new_s + len(word)])\n",
    "        se2idx[f\"{new_s}-{new_s + len(word)}\"] = i\n",
    "        word_list.append(word)\n",
    "        old_s += len(word) + 1\n",
    "        \n",
    "    candidates = []\n",
    "        \n",
    "    def complete_dataset(dataset_idx, word_list, mode=\"backward\"):\n",
    "        # go backwards until meet 2 consecutive lowercase words\n",
    "        candidate_word = []\n",
    "        n_consecutive_lower = 0\n",
    "        if mode == \"backward\":\n",
    "            for i in range(dataset_idx, 0, -1):\n",
    "                word_i = word_list[i]\n",
    "                if word_i.islower() is False and word_i.replace(\"\\'\", \"\").isalpha():\n",
    "                    candidate_word.append(word_i)\n",
    "                    n_consecutive_lower = 0 # reset it to 0\n",
    "                else:\n",
    "                    # temporal add this word, will remove\n",
    "                    # if n_consecutive_lower == 2\n",
    "                    n_consecutive_lower += 1\n",
    "                    if n_consecutive_lower == 2 or word_i.lower() in [\"a\", \"an\", \"the\"]:\n",
    "                        if n_consecutive_lower == 2:\n",
    "                            candidate_word = candidate_word[:-1]\n",
    "                        break\n",
    "                    else:\n",
    "                        candidate_word.append(word_i)\n",
    "                        \n",
    "            # remove a, an, the\n",
    "            if candidate_word[-1] in [\"A\", \"An\", \"The\"]:\n",
    "                candidate_word = candidate_word[:-1]\n",
    "            return \" \".join(candidate_word[::-1])\n",
    "        else:\n",
    "            for i in range(dataset_idx, len(word_list), 1):\n",
    "                word_i = word_list[i]\n",
    "                if word_i.islower() is False and word_i.replace(\"\\'\", \"\").isalpha():\n",
    "                    candidate_word.append(word_i)\n",
    "                    n_consecutive_lower = 0 # reset it to 0\n",
    "                else:\n",
    "                    # temporal add this word, will remove\n",
    "                    # if n_consecutive_lower == 2\n",
    "                    n_consecutive_lower += 1\n",
    "                    if n_consecutive_lower == 2 or word_i.lower() in [\"a\", \"an\", \"the\"]:\n",
    "                        if n_consecutive_lower == 2:\n",
    "                            candidate_word = candidate_word[:-1]\n",
    "                        break\n",
    "                    else:\n",
    "                        candidate_word.append(word_i)\n",
    "\n",
    "            return \" \".join(candidate_word)\n",
    "\n",
    "    # forward, backward complete (keyword + prepositions)\n",
    "    for k in keywords:\n",
    "        for prepos in prepositions:\n",
    "            new_k = \" \" + k + f\" {prepos} \"\n",
    "            matchs = re.finditer(new_k, string)\n",
    "            for match in matchs:\n",
    "                start_index = match.start() + 1\n",
    "                end_index = match.end() - 1\n",
    "                forward_candidate = complete_dataset(se2idx[f\"{start_index}-{start_index + len(k)}\"] ,word_list, \"forward\")\n",
    "                backward_candidate = complete_dataset(se2idx[f\"{start_index}-{start_index + len(k)}\"] ,word_list, \"backward\")\n",
    "                candidate = \" \".join(backward_candidate.split(\" \") + [prepos] + forward_candidate.split(\" \")[2:])\n",
    "                words = candidate.split()\n",
    "                words = [w for i,w in enumerate(words) if w not in prepositions or i == len(words) - 1]\n",
    "                acronym = \"\".join([w[0] for w in words])\n",
    "                if len(candidate.split(\" \")) >= 3 and len(candidate.split(\" \")) <= 10 and acronym.isupper() and len(candidate) <= 60 and check_splcharacter(candidate):\n",
    "                    candidates.append(candidate)\n",
    "    \n",
    "    # backward complete\n",
    "    for k in keywords:\n",
    "        matchs = re.finditer(\" \"+ k + \" \", string)\n",
    "        for match in matchs:\n",
    "            start_index = match.start() + 1\n",
    "            end_index = match.end() - 1\n",
    "            candidate = complete_dataset(se2idx[f\"{start_index}-{end_index}\"], word_list, \"backward\")\n",
    "            words = candidate.split()\n",
    "            words = [w for i,w in enumerate(words) if w not in prepositions or i == len(words) - 1]\n",
    "            acronym = \"\".join([w[0] for w in words])\n",
    "            if len(candidate.split(\" \")) >= 3 and len(candidate.split(\" \")) <= 10 and acronym.isupper() and len(candidate) <= 60 and check_splcharacter(candidate):\n",
    "                candidates.append(candidate)\n",
    "                \n",
    "    # forward complete\n",
    "    for k in keywords:\n",
    "        matchs = re.finditer(\" \"+ k + \" \", string)\n",
    "        for match in matchs:\n",
    "            start_index = match.start() + 1\n",
    "            end_index = match.end() - 1\n",
    "            candidate = complete_dataset(se2idx[f\"{start_index}-{end_index}\"], word_list, \"forward\")\n",
    "            words = candidate.split()\n",
    "            words = [w for i,w in enumerate(words) if w not in prepositions or i == len(words) - 1]\n",
    "            acronym = \"\".join([w[0] for w in words])\n",
    "            if len(candidate.split(\" \")) >= 3 and len(candidate.split(\" \")) <= 10 and acronym.isupper() and len(candidate) <= 60 and check_splcharacter(candidate):\n",
    "                candidates.append(candidate)\n",
    "                \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.352502,
     "end_time": "2021-06-26T01:19:23.112439",
     "exception": false,
     "start_time": "2021-06-26T01:19:22.759937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:19:23.820143Z",
     "iopub.status.busy": "2021-06-26T01:19:23.819309Z",
     "iopub.status.idle": "2021-06-26T01:19:23.823464Z",
     "shell.execute_reply": "2021-06-26T01:19:23.822769Z",
     "shell.execute_reply.started": "2021-06-26T00:57:03.997662Z"
    },
    "papermill": {
     "duration": 0.359431,
     "end_time": "2021-06-26T01:19:23.823646",
     "exception": false,
     "start_time": "2021-06-26T01:19:23.464215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_accepted_preds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:19:24.532750Z",
     "iopub.status.busy": "2021-06-26T01:19:24.531884Z",
     "iopub.status.idle": "2021-06-26T01:19:24.536908Z",
     "shell.execute_reply": "2021-06-26T01:19:24.536257Z",
     "shell.execute_reply.started": "2021-06-26T00:57:04.078928Z"
    },
    "papermill": {
     "duration": 0.363699,
     "end_time": "2021-06-26T01:19:24.537057",
     "exception": false,
     "start_time": "2021-06-26T01:19:24.173358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def iterator_data(items_list):\n",
    "    for item in items_list:\n",
    "        yield item\n",
    "\n",
    "def get_extra_label(string):\n",
    "    candidates0 = custom_find_dataset0(string)\n",
    "    candidates1 = custom_find_dataset1(string)\n",
    "    candidates2 = custom_find_dataset2(string)\n",
    "    return candidates0 + candidates1 + candidates2\n",
    "\n",
    "iterator_data = iterator_data(full_texts)\n",
    "partial_fn = partial(get_extra_label)\n",
    "p = Pool(16)\n",
    "extra_label_results = p.imap(\n",
    "    partial_fn,\n",
    "    tqdm(iterator_data, total=len(full_texts), desc=\"[Get extra labels]\"),\n",
    "    chunksize=10,\n",
    ")\n",
    "\n",
    "for result in tqdm(extra_label_results):\n",
    "    custom_accepted_preds.extend(result)\n",
    "    \n",
    "custom_accepted_preds = list(set(custom_accepted_preds))\n",
    "\n",
    "p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(custom_accepted_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Bad Labels from Scispacy and A Custom Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:19:25.954494Z",
     "iopub.status.busy": "2021-06-26T01:19:25.953857Z",
     "iopub.status.idle": "2021-06-26T01:19:25.957375Z",
     "shell.execute_reply": "2021-06-26T01:19:25.956870Z",
     "shell.execute_reply.started": "2021-06-26T00:57:04.206357Z"
    },
    "papermill": {
     "duration": 0.355613,
     "end_time": "2021-06-26T01:19:25.957501",
     "exception": false,
     "start_time": "2021-06-26T01:19:25.601888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "accepted_preds_counter = Counter(accepted_preds)\n",
    "custom_accepted_preds_counter = Counter(custom_accepted_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.352515,
     "end_time": "2021-06-26T01:19:26.664321",
     "exception": false,
     "start_time": "2021-06-26T01:19:26.311806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:19:27.368650Z",
     "iopub.status.busy": "2021-06-26T01:19:27.368016Z",
     "iopub.status.idle": "2021-06-26T01:19:27.370279Z",
     "shell.execute_reply": "2021-06-26T01:19:27.369648Z",
     "shell.execute_reply.started": "2021-06-26T00:57:04.256854Z"
    },
    "papermill": {
     "duration": 0.354286,
     "end_time": "2021-06-26T01:19:27.370405",
     "exception": false,
     "start_time": "2021-06-26T01:19:27.016119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def num_there(s):\n",
    "    return any(i.isdigit() for i in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:19:27.959482Z",
     "iopub.status.busy": "2021-06-26T01:19:27.958806Z",
     "iopub.status.idle": "2021-06-26T01:19:27.962834Z",
     "shell.execute_reply": "2021-06-26T01:19:27.963255Z",
     "shell.execute_reply.started": "2021-06-26T00:57:04.293218Z"
    },
    "papermill": {
     "duration": 0.236218,
     "end_time": "2021-06-26T01:19:27.963406",
     "exception": false,
     "start_time": "2021-06-26T01:19:27.727188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "accepted_preds = []\n",
    "\n",
    "# atleast 3 words for scispacy\n",
    "for k, v in accepted_preds_counter.items():\n",
    "    if v >= 1 and num_there(k) is False and len(clean_text(k).strip().split(\" \")) >= 3 and len(clean_text(k).strip().split(\" \")) <= 10 and check_splcharacter(k):\n",
    "        accepted_preds.append(k.strip())\n",
    "\n",
    "# atleast 4 words for our custom algorithm (optional, you can ingore those labels)\n",
    "for k, v in custom_accepted_preds_counter.items():\n",
    "    if v >= 1 and num_there(k) is False and len(clean_text(k).strip().split(\" \")) >= 4 and len(clean_text(k).strip().split(\" \")) <= 10 and check_splcharacter(k):\n",
    "        accepted_preds.append(k.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:19:28.418276Z",
     "iopub.status.busy": "2021-06-26T01:19:28.417609Z",
     "iopub.status.idle": "2021-06-26T01:19:28.421267Z",
     "shell.execute_reply": "2021-06-26T01:19:28.421750Z",
     "shell.execute_reply.started": "2021-06-26T00:57:04.334882Z"
    },
    "papermill": {
     "duration": 0.231239,
     "end_time": "2021-06-26T01:19:28.421884",
     "exception": false,
     "start_time": "2021-06-26T01:19:28.190645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "accepted_preds = list(set(accepted_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:19:28.875442Z",
     "iopub.status.busy": "2021-06-26T01:19:28.874776Z",
     "iopub.status.idle": "2021-06-26T01:19:28.876871Z",
     "shell.execute_reply": "2021-06-26T01:19:28.877392Z",
     "shell.execute_reply.started": "2021-06-26T00:57:04.390146Z"
    },
    "papermill": {
     "duration": 0.230043,
     "end_time": "2021-06-26T01:19:28.877563",
     "exception": false,
     "start_time": "2021-06-26T01:19:28.647520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(accepted_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-26T01:19:29.581706Z",
     "iopub.status.busy": "2021-06-26T01:19:29.581192Z",
     "iopub.status.idle": "2021-06-26T01:19:29.583474Z",
     "shell.execute_reply": "2021-06-26T01:19:29.583930Z",
     "shell.execute_reply.started": "2021-06-26T00:57:04.420167Z"
    },
    "papermill": {
     "duration": 0.356099,
     "end_time": "2021-06-26T01:19:29.584070",
     "exception": false,
     "start_time": "2021-06-26T01:19:29.227971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Valid Extra Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.362572,
     "end_time": "2021-06-26T01:19:40.119774",
     "exception": false,
     "start_time": "2021-06-26T01:19:39.757202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds_not_in_train = []\n",
    "for pred in tqdm(accepted_preds):\n",
    "    in_train = False\n",
    "    for train_label in all_train_labels:\n",
    "        clean_pred = clean_text(pred)\n",
    "        clean_train = clean_text(train_label)\n",
    "        jaccard = jaccard_similarity(clean_pred, clean_train)\n",
    "        if jaccard >= 0.5 or clean_pred in clean_train or clean_train in clean_pred:\n",
    "            in_train = True\n",
    "            break\n",
    "    if in_train is False:\n",
    "        preds_not_in_train.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_in_train = list(set(accepted_preds) - set(preds_not_in_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_valid_extra_label(extra_label):\n",
    "    \"\"\"\n",
    "    This function check if the extra label statisfy\n",
    "    all below condition or not.\n",
    "    \"\"\"\n",
    "    accepted_keywords = [\"Study\", \"Studies\", \"Survey\", \"Surveys\"]\n",
    "    accepted_keywords2 = [\"Dataset\", \"Database\", \"Datasets\", \"Databases\"] # \"Program\", \"Programs\", \"Programe\"\n",
    "    accepted_keywords3 = [\"Data Set\", \"Data System\", \"Data Systems\", \"Data Sets\", \"Dataset System\", \"Dataset Systems\"]\n",
    "    words = extra_label.split(\" \")\n",
    "    first_word = words[0]\n",
    "    last_word = words[-1]\n",
    "    for k in accepted_keywords:\n",
    "        if (\n",
    "            (\n",
    "                k in first_word\n",
    "                or k in last_word\n",
    "                or ((k + \" on\") in extra_label)\n",
    "                or ((k + \" for\") in extra_label)\n",
    "                or ((k + \" of\") in extra_label)\n",
    "                or ((k + \" in\") in extra_label)\n",
    "            )\n",
    "            and first_word[0].isupper()\n",
    "            and last_word[0].isupper()\n",
    "        ):\n",
    "            return True\n",
    "    for k in accepted_keywords2:\n",
    "        if (\n",
    "            (\n",
    "                k in last_word\n",
    "                or ((k + \" on\") in extra_label)\n",
    "                or ((k + \" for\") in extra_label)\n",
    "                or ((k + \" of\") in extra_label)\n",
    "                or ((k + \" in\") in extra_label)\n",
    "            )\n",
    "            and first_word[0].isupper()\n",
    "        ):\n",
    "            return True\n",
    "    for k in accepted_keywords3:\n",
    "        if k in \" \".join(words[-2:]) and first_word[0].isupper():\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(settings[\"RAW_DATA_DIR\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create extra_train_labels and save it into ./pretrained\n",
    "pseudo_labels = []\n",
    "for label in preds_in_train:\n",
    "    if check_valid_extra_label(label):\n",
    "        pseudo_labels.append(label)\n",
    "pseudo = pd.DataFrame()\n",
    "pseudo[\"label\"] = pseudo_labels\n",
    "pseudo.to_csv(f\"{settings['RAW_DATA_DIR']}/extra_train_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create extra_valid_labels and save it into ./pretrained\n",
    "pseudo_labels = []\n",
    "for label in preds_not_in_train:\n",
    "    if check_valid_extra_label(label):\n",
    "        pseudo_labels.append(label)\n",
    "pseudo = pd.DataFrame()\n",
    "pseudo[\"label\"] = pseudo_labels\n",
    "pseudo.to_csv(f\"{settings['RAW_DATA_DIR']}/extra_valid_labels.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 853.46971,
   "end_time": "2021-06-26T01:19:46.766075",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-26T01:05:33.296365",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
