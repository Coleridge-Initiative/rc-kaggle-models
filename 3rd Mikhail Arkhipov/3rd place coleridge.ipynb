{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014639,
     "end_time": "2021-06-28T13:07:17.638910",
     "exception": false,
     "start_time": "2021-06-28T13:07:17.624271",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Brief Solution Description\n",
    "\n",
    "The solution is based on a simple heuristic: a capitalized sequence of words that includes a keyword and followed by parenthesis usually refer to a dataset. So, any sequence like \n",
    "\n",
    "``` Xxx Xxx Keyword Xxx (XXX)```\n",
    "\n",
    "is a good candidate to be a dataset.\n",
    "\n",
    "All mentions of a given form are extracted to form a list of dataset names to look for. Each text in the test is checked for inclusion of the dataset name from the list. Every match is added to the prediction. Substring predictions are removed.\n",
    "\n",
    "Keywords list:\n",
    "\n",
    "- Study\n",
    "- Survey\n",
    "- Assessment\n",
    "- Initiative\n",
    "- Data\n",
    "- Dataset\n",
    "- Database\n",
    "\n",
    "Also, many data mentions refer to some organizations or systems. These mentions seem to be non-valid dataset names. To remove them the following list of stopwords is used:\n",
    "\n",
    "``` \n",
    "' lab', 'centre', 'center', 'consortium', 'office', 'agency', 'administration', 'clearinghouse',\n",
    "'corps', 'organization', 'organisation', 'association', 'university', 'department',\n",
    "'institute', 'foundation', 'service', 'bureau', 'company', 'test', 'tool', 'board', 'scale',\n",
    "'framework', 'committee', 'system', 'group', 'rating', 'manual', 'division', 'supplement',\n",
    "'variables', 'documentation', 'format' \n",
    "```\n",
    "\n",
    " To exclude mentions not related to data a simple count statistic is used: \n",
    " \n",
    " $$ F_d = \\frac{N_{data}(str)}{N_{total}(str)}$$\n",
    " \n",
    " where $N_{data}(str)$ is the number of times the `str` occures with `data` word (parenthesis are dropped) and $N_{total}(str)$ is the total number of times `str` present in texts. All mentions with $F_d < 0.1$ are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-28T13:07:17.673244Z",
     "iopub.status.busy": "2021-06-28T13:07:17.672545Z",
     "iopub.status.idle": "2021-06-28T13:07:17.675085Z",
     "shell.execute_reply": "2021-06-28T13:07:17.675604Z",
     "shell.execute_reply.started": "2021-06-28T12:54:17.933757Z"
    },
    "papermill": {
     "duration": 0.023573,
     "end_time": "2021-06-28T13:07:17.675959",
     "exception": false,
     "start_time": "2021-06-28T13:07:17.652386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import json\n",
    "from itertools import chain, combinations\n",
    "from typing import Callable, List, Union, Optional, Set, Dict\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-28T13:07:17.709999Z",
     "iopub.status.busy": "2021-06-28T13:07:17.709291Z",
     "iopub.status.idle": "2021-06-28T13:07:17.714867Z",
     "shell.execute_reply": "2021-06-28T13:07:17.714164Z",
     "shell.execute_reply.started": "2021-06-28T12:54:18.352931Z"
    },
    "papermill": {
     "duration": 0.025549,
     "end_time": "2021-06-28T13:07:17.715030",
     "exception": false,
     "start_time": "2021-06-28T13:07:17.689481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TOKENIZE_PAT = re.compile(\"[\\w']+|[^\\w ]\")\n",
    "CAMEL_PAT = re.compile(r'(\\b[A-Z]+[a-z]+[A-Z]\\w+)')\n",
    "BR_PAT = re.compile('\\s?\\((.*)\\)')\n",
    "PREPS = {'from', 'for', 'of', 'the', 'in', 'with', 'to', 'on', 'and'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-28T13:07:17.748494Z",
     "iopub.status.busy": "2021-06-28T13:07:17.747623Z",
     "iopub.status.idle": "2021-06-28T13:07:17.751060Z",
     "shell.execute_reply": "2021-06-28T13:07:17.750380Z",
     "shell.execute_reply.started": "2021-06-28T12:54:18.518114Z"
    },
    "papermill": {
     "duration": 0.022754,
     "end_time": "2021-06-28T13:07:17.751220",
     "exception": false,
     "start_time": "2021-06-28T13:07:17.728466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return TOKENIZE_PAT.findall(text)\n",
    "\n",
    "\n",
    "def clean_text(txt):\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013424,
     "end_time": "2021-06-28T13:07:17.781800",
     "exception": false,
     "start_time": "2021-06-28T13:07:17.768376",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-28T13:07:17.835338Z",
     "iopub.status.busy": "2021-06-28T13:07:17.828281Z",
     "iopub.status.idle": "2021-06-28T13:07:17.857689Z",
     "shell.execute_reply": "2021-06-28T13:07:17.858186Z",
     "shell.execute_reply.started": "2021-06-28T12:54:18.818779Z"
    },
    "papermill": {
     "duration": 0.059821,
     "end_time": "2021-06-28T13:07:17.858416",
     "exception": false,
     "start_time": "2021-06-28T13:07:17.798595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_substring_predictions(preds):\n",
    "    preds = set(preds)\n",
    "    to_filter = set()\n",
    "    for d1, d2 in combinations(preds, 2):\n",
    "        if d1 in d2:\n",
    "            to_filter.add(d1)\n",
    "        if d2 in d1:\n",
    "            to_filter.add(d2)\n",
    "    return list(preds - to_filter)\n",
    "\n",
    "\n",
    "def filter_stopwords(datasets, stopwords, do_lower=True):\n",
    "    # Remove all instances that contain any stopword as a substring\n",
    "    filtered_datasets = []\n",
    "    if do_lower:\n",
    "        stopwords = [sw.lower() for sw in stopwords]\n",
    "    for ds in datasets:\n",
    "        ds_to_analyze = ds.lower() if do_lower else ds\n",
    "        if any(sw in ds_to_analyze for sw in stopwords):\n",
    "            continue\n",
    "        filtered_datasets.append(ds)\n",
    "    return filtered_datasets\n",
    "\n",
    "\n",
    "def extend_parentehis(datasets):\n",
    "    # Return each instance of dataset from datasets + \n",
    "    # the same instance without parenthesis (if there are some)\n",
    "    pat = re.compile('\\(.*\\)')\n",
    "    extended_datasets = []\n",
    "    for ds in datasets:\n",
    "        ds_no_parenth = pat.sub('', ds).strip()\n",
    "        if ds != ds_no_parenth:\n",
    "            extended_datasets.append(ds_no_parenth)\n",
    "        extended_datasets.append(ds)\n",
    "    return extended_datasets\n",
    "\n",
    "\n",
    "def filler_intro_words(datasets):\n",
    "    miss_intro_pat = re.compile('^[A-Z][a-z\\']+ (?:the|to the) ')\n",
    "    return [miss_intro_pat.sub('', ds) for ds in datasets]\n",
    "\n",
    "\n",
    "def filter_parial_match_datasets(datasets):\n",
    "    # Some matches are truncated due to parsing errors \n",
    "    # or other factors. To remove those, we look for \n",
    "    # the most common form of the dataset and remove\n",
    "    # mentions, that are substrings of this form.\n",
    "    # Obviously, some true mentions might be dropped \n",
    "    # at this stage\n",
    "    counter = Counter(datasets)\n",
    "\n",
    "    abbrs_used = set()\n",
    "    golden_ds_with_br = []\n",
    "\n",
    "    for ds, count in counter.most_common():\n",
    "        abbr = BR_PAT.findall(ds)[0]\n",
    "        no_br_ds = BR_PAT.sub('', ds)\n",
    "\n",
    "        if abbr not in abbrs_used:\n",
    "            abbrs_used.add(abbr)\n",
    "            golden_ds_with_br.append(ds)\n",
    "\n",
    "    filtered_datasets = []\n",
    "    for ds in datasets:\n",
    "        if not any((ds in ds_) and (ds != ds_) for ds_ in golden_ds_with_br):\n",
    "            filtered_datasets.append(ds)\n",
    "    return filtered_datasets\n",
    "\n",
    "\n",
    "def filter_br_less_than_two_words(datasets):\n",
    "    filtered_datasets = []\n",
    "    for ds in datasets:\n",
    "        no_br_ds = BR_PAT.sub('', ds)\n",
    "        if len(tokenize(no_br_ds)) > 2:\n",
    "            filtered_datasets.append(ds)\n",
    "    return filtered_datasets\n",
    "\n",
    "\n",
    "def filter_intro_ssai(datasets):\n",
    "    # Filtering introductory words marked as a part of the mention by mistake\n",
    "    connection_words = {'of', 'the', 'with', 'for', 'in', 'to', 'on', 'and', 'up'}\n",
    "    keywords = {'Program', 'Study', 'Survey', 'Assessment'}\n",
    "    filtered_datasets = []\n",
    "    for ds in datasets:\n",
    "        toks_spans = list(TOKENIZE_PAT.finditer(ds))\n",
    "        toks = [t.group() for t in toks_spans]\n",
    "        start = 0\n",
    "        if len(toks) > 3:\n",
    "            if toks[1] == 'the':\n",
    "                start = toks_spans[2].span()[0]\n",
    "            elif toks[0] not in keywords and  toks[1] in connection_words and len(toks) > 2 and toks[2] in connection_words:\n",
    "                start = toks_spans[3].span()[0]\n",
    "            elif toks[0].endswith('ing') and toks[1] in connection_words:\n",
    "                if toks[2] not in connection_words:\n",
    "                    start_tok = 2\n",
    "                else:\n",
    "                    start_tok = 3\n",
    "                start = toks_spans[start_tok].span()[0]\n",
    "            filtered_datasets.append(ds[start:])\n",
    "        else:\n",
    "            filtered_datasets.append(ds)\n",
    "    return filtered_datasets\n",
    "\n",
    "\n",
    "def get_index(texts: List[str], words: List[str]) -> Dict[str, Set[int]]:\n",
    "    # Returns a dictionary where words are keys and values are indices \n",
    "    # of documents (sentences) in texts, in which the word present\n",
    "    index = defaultdict(set)\n",
    "    words = set(words)\n",
    "    words = {w for w in words if w.lower() not in PREPS and re.sub('\\'', '', w).isalnum()}\n",
    "    for n, text in tqdm(enumerate(texts), total=len(texts)):\n",
    "        tokens = tokenize(text)\n",
    "        for tok in tokens:\n",
    "            if tok in words:\n",
    "                index[tok].add(n)\n",
    "    return index\n",
    "\n",
    "\n",
    "def get_train_predictions_counts_data(datasets, index, kw):\n",
    "    # Returns N_data and N_total counts dictionary \n",
    "    # (check the formulas in the first cell)\n",
    "    pred_count = Counter()\n",
    "    data_count = Counter()\n",
    "    if isinstance(kw, str):\n",
    "        kw = [kw]\n",
    "    \n",
    "    for ds in tqdm(datasets):\n",
    "        first_tok, *toks = tokenize(ds)\n",
    "        to_search = None\n",
    "        for tok in [first_tok] + toks:\n",
    "            if index.get(tok):\n",
    "                if to_search is None:\n",
    "                    to_search = set(index[tok])\n",
    "                else:\n",
    "                    to_search &= index[tok]\n",
    "        for doc_idx in to_search:\n",
    "            text = texts[doc_idx]\n",
    "            if ds in text:\n",
    "                pred_count[ds] += 1\n",
    "                data_count[ds] += int(any(w in text.lower() for w in kw))\n",
    "    return pred_count, data_count\n",
    "\n",
    "\n",
    "def filter_by_train_counts(datasets, index, kw, min_train_count, rel_freq_threshold):\n",
    "    # Filter by relative frequency (no parenthesis)\n",
    "    # (check the formula in the first cell)\n",
    "    tr_counts, data_counts = get_train_predictions_counts_data(extend_parentehis(set(datasets)), index, kw)\n",
    "    stats = []\n",
    "\n",
    "    for ds, count in Counter(datasets).most_common():\n",
    "        stats.append([ds, count, tr_counts[ds], tr_counts[re.sub('[\\s]?\\(.*\\)', '', ds)],\n",
    "                      data_counts[ds], data_counts[re.sub('[\\s]?\\(.*\\)', '', ds)]])\n",
    "    \n",
    "    filtered_datasets = []\n",
    "    for ds, count, tr_count, tr_count_no_br, dcount, dcount_nobr in stats:\n",
    "        if (tr_count_no_br > min_train_count) and (dcount_nobr / tr_count_no_br > rel_freq_threshold):\n",
    "            filtered_datasets.append(ds)\n",
    "    return filtered_datasets\n",
    "\n",
    "\n",
    "def filter_and_the(datasets):\n",
    "    pat = re.compile(' and [Tt]he ')\n",
    "    return [pat.split(ds)[-1] for ds in datasets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-28T13:07:17.890272Z",
     "iopub.status.busy": "2021-06-28T13:07:17.889220Z",
     "iopub.status.idle": "2021-06-28T13:07:17.893545Z",
     "shell.execute_reply": "2021-06-28T13:07:17.894003Z",
     "shell.execute_reply.started": "2021-06-28T12:54:18.966114Z"
    },
    "papermill": {
     "duration": 0.02212,
     "end_time": "2021-06-28T13:07:17.894208",
     "exception": false,
     "start_time": "2021-06-28T13:07:17.872088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/')\n",
    "TRAIN_MARKUP_FILE = DATA_DIR / 'train.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014691,
     "end_time": "2021-06-28T13:07:17.922389",
     "exception": false,
     "start_time": "2021-06-28T13:07:17.907698",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-28T13:07:17.959262Z",
     "iopub.status.busy": "2021-06-28T13:07:17.958344Z",
     "iopub.status.idle": "2021-06-28T13:07:17.979359Z",
     "shell.execute_reply": "2021-06-28T13:07:17.980024Z",
     "shell.execute_reply.started": "2021-06-28T12:54:19.251081Z"
    },
    "papermill": {
     "duration": 0.042951,
     "end_time": "2021-06-28T13:07:17.980231",
     "exception": false,
     "start_time": "2021-06-28T13:07:17.937280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Sentencizer:\n",
    "    def __init__(self,\n",
    "                 sentencize_fun: Callable,\n",
    "                 split_by_newline: bool = True) -> None:\n",
    "        self.sentencize = sentencize_fun\n",
    "        self.split_by_newline = split_by_newline\n",
    "\n",
    "    def __call__(self, text: str) -> List[str]:\n",
    "        if self.split_by_newline:\n",
    "            texts = text.split('\\n')\n",
    "        else:\n",
    "            texts = [text]\n",
    "        sents = []\n",
    "        for text in texts:\n",
    "            sents.extend(self.sentencize(text))\n",
    "        return sents\n",
    "\n",
    "\n",
    "class DotSplitSentencizer(Sentencizer):\n",
    "    def __init__(self,\n",
    "                 split_by_newline: bool) -> None:\n",
    "        def _sent_fun(text: str) -> List[str]:\n",
    "            return [sent.strip() for sent in text.split('.') if sent]\n",
    "        super().__init__(_sent_fun, split_by_newline)\n",
    "\n",
    "\n",
    "def get_coleridge_data(data_path: Union[str, Path],\n",
    "                       sentencizer: Optional[Sentencizer] = None) -> None:\n",
    "    data_path = Path(data_path)\n",
    "\n",
    "    df = pd.read_csv(data_path / 'train.csv')\n",
    "\n",
    "    samples = {}\n",
    "    for _, (idx, pub_title, dataset_title, dataset_label, cleaned_label) in tqdm(df.iterrows()):\n",
    "        if idx not in samples:\n",
    "            with open(data_path / 'train' / (idx + '.json')) as fp:\n",
    "                data = json.load(fp)\n",
    "            samples[idx] = {'texts': [sec['text'] for sec in data], \n",
    "                            'dataset_titles': [],\n",
    "                            'dataset_labels': [],\n",
    "                            'cleaned_labels': [],\n",
    "                            'pub_title': pub_title,\n",
    "                            'idx': idx\n",
    "                            }\n",
    "        samples[idx]['dataset_titles'].append(dataset_title)\n",
    "        samples[idx]['dataset_labels'].append(dataset_label)\n",
    "        samples[idx]['cleaned_labels'].append(cleaned_label)\n",
    "\n",
    "    train_ids = []\n",
    "    train_texts = []\n",
    "    train_labels = []\n",
    "    for sample_dict in samples.values():\n",
    "        train_ids.append(sample_dict['idx'])\n",
    "        texts = sample_dict['texts']\n",
    "        if sentencizer is not None:\n",
    "            texts = list(chain(*[sentencizer(text) for text in texts]))\n",
    "        train_texts.append(texts)\n",
    "        train_labels.append(sample_dict['dataset_labels'])\n",
    "    \n",
    "    test_texts = []\n",
    "    test_ids = []\n",
    "    for test_file in (data_path / 'test').glob('*.json'):\n",
    "        idx = test_file.name.split('.')[0]\n",
    "        with open(test_file) as fp:\n",
    "            data = json.load(fp)\n",
    "        texts = [sec['text'] for sec in data]\n",
    "        if sentencizer is not None:\n",
    "            texts = list(chain(*[sentencizer(text) for text in texts]))\n",
    "\n",
    "        test_texts.append(texts)\n",
    "        test_ids.append(idx)\n",
    "        \n",
    "    return train_texts, train_ids, train_labels, test_texts, test_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-28T13:07:18.012331Z",
     "iopub.status.busy": "2021-06-28T13:07:18.011617Z",
     "iopub.status.idle": "2021-06-28T13:09:00.412611Z",
     "shell.execute_reply": "2021-06-28T13:09:00.411849Z",
     "shell.execute_reply.started": "2021-06-28T12:54:19.421920Z"
    },
    "papermill": {
     "duration": 102.418514,
     "end_time": "2021-06-28T13:09:00.412796",
     "exception": false,
     "start_time": "2021-06-28T13:07:17.994282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19661it [01:36, 204.38it/s]\n"
     ]
    }
   ],
   "source": [
    "train_texts, train_ids, train_labels, test_texts, test_ids = get_coleridge_data(DATA_DIR, DotSplitSentencizer(True))\n",
    "train_labels_set = set(chain(*train_labels))\n",
    "\n",
    "# all sentences from train and test as a single list\n",
    "texts = list(chain(*(train_texts + test_texts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.259595,
     "end_time": "2021-06-28T13:09:00.932622",
     "exception": false,
     "start_time": "2021-06-28T13:09:00.673027",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pattern Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-28T13:09:01.822659Z",
     "iopub.status.busy": "2021-06-28T13:09:01.813549Z",
     "iopub.status.idle": "2021-06-28T13:09:01.826679Z",
     "shell.execute_reply": "2021-06-28T13:09:01.825953Z",
     "shell.execute_reply.started": "2021-06-28T13:01:21.652739Z"
    },
    "papermill": {
     "duration": 0.63248,
     "end_time": "2021-06-28T13:09:01.826852",
     "exception": false,
     "start_time": "2021-06-28T13:09:01.194372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenzed_extract(texts, keywords):\n",
    "    # Exracts all mentions of the form\n",
    "    # Xxx Xxx Keyword Xxx (XXX)\n",
    "    \n",
    "    connection_words = {'of', 'the', 'with', 'for', 'in', 'to', 'on', 'and', 'up'}\n",
    "    datasets = []\n",
    "    for text in tqdm(texts):\n",
    "        try:\n",
    "            # Skip texts without parenthesis orXxx Xxx Keyword Xxx (XXX) keywords\n",
    "            if '(' not in text or all(not kw in text for kw in keywords):\n",
    "                continue\n",
    "\n",
    "            toks = list(TOKENIZE_PAT.finditer(text))\n",
    "            toksg = [tok.group() for tok in toks]\n",
    "\n",
    "            found = False\n",
    "            current_dss = set()\n",
    "            for n in range(1, len(toks) - 2):\n",
    "                is_camel = bool(CAMEL_PAT.findall(toksg[n + 1]))\n",
    "                is_caps = toksg[n + 1].isupper()\n",
    "                \n",
    "                if toksg[n] == '(' and (is_caps or is_camel) and toksg[n + 2] == ')':\n",
    "                    end = toks[n + 2].span()[1]\n",
    "                    n_capi = 0\n",
    "                    has_kw = False\n",
    "                    for tok, tokg in zip(toks[n - 1:: -1], toksg[n - 1:: -1]):\n",
    "                        if tokg in keywords:\n",
    "                            has_kw = True\n",
    "                        if tokg[0].isupper() and tokg.lower() not in connection_words:\n",
    "                            n_capi += 1\n",
    "                            start = tok.span()[0]\n",
    "                        elif tokg in connection_words or tokg == '-':\n",
    "                            continue\n",
    "                        else:\n",
    "                            break\n",
    "                    if n_capi > 1 and has_kw:\n",
    "                        ds = text[start: end]\n",
    "                        datasets.append(ds)\n",
    "                        found = True\n",
    "                        current_dss.add(ds)\n",
    "        except:\n",
    "            print(text)\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def get_parenthesis(t, ds):\n",
    "    # Get abbreviations in the brackets if there are any \n",
    "    cur_abbrs = re.findall(re.escape(ds) + '\\s?(\\([^\\)]+\\)|\\[[^\\]]+\\])', t)\n",
    "    cur_abbrs = [abbr.strip('()[]').strip() for abbr in cur_abbrs]\n",
    "    cur_abbrs = [re.split('[\\(\\[]', abbr)[0].strip() for abbr in cur_abbrs]\n",
    "    cur_abbrs = [re.split('[;,]', abbr)[0].strip() for abbr in cur_abbrs]\n",
    "    cur_abbrs = [a for a in cur_abbrs if not any(ch in a for ch in '[]()')]\n",
    "    cur_abbrs = [a for a in cur_abbrs if re.findall('[A-Z][A-Z]', a)]\n",
    "    cur_abbrs = [a for a in cur_abbrs if len(a) > 2]\n",
    "    cur_abbrs = [a for a in cur_abbrs if not any(tok.islower() for tok in tokenize(a))]\n",
    "    fabbrs = []\n",
    "    for abbr in cur_abbrs:\n",
    "        if not (sum(bool(re.findall('[A-Z][a-z]+', tok)) for tok in tokenize(abbr)) > 2):\n",
    "            fabbrs.append(abbr)\n",
    "    return fabbrs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.258727,
     "end_time": "2021-06-28T13:09:02.348526",
     "exception": false,
     "start_time": "2021-06-28T13:09:02.089799",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-28T13:09:02.879706Z",
     "iopub.status.busy": "2021-06-28T13:09:02.878530Z",
     "iopub.status.idle": "2021-06-28T13:09:02.882041Z",
     "shell.execute_reply": "2021-06-28T13:09:02.881369Z",
     "shell.execute_reply.started": "2021-06-28T13:03:38.610854Z"
    },
    "papermill": {
     "duration": 0.274457,
     "end_time": "2021-06-28T13:09:02.882198",
     "exception": false,
     "start_time": "2021-06-28T13:09:02.607741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_datasets():\n",
    "    STOPWORDS_PAR = [' lab', 'centre', 'center', 'consortium', 'office', 'agency', 'administration', 'clearinghouse',\n",
    "                     'corps', 'organization', 'organisation', 'association', 'university', 'department',\n",
    "                     'institute', 'foundation', 'service', 'bureau', 'company', 'test', 'tool', 'board', 'scale',\n",
    "                     'framework', 'committee', 'system', 'group', 'rating', 'manual', 'division', 'supplement',\n",
    "                     'variables', 'documentation', 'format']\n",
    "\n",
    "    filter_stopwords_par_data = partial(filter_stopwords, stopwords=STOPWORDS_PAR)\n",
    "\n",
    "    keywords = {'Study', 'Survey', 'Assessment', 'Initiative', 'Data', 'Dataset', 'Database'}\n",
    "    \n",
    "    # Datasets \n",
    "    ssai_par_datasets = tokenzed_extract(texts, keywords)\n",
    "    \n",
    "    words = list(chain(*[tokenize(ds) for ds in ssai_par_datasets]))\n",
    "    texts_index = get_index(texts, words)\n",
    "    filter_by_train_counts_filled = partial(filter_by_train_counts, index=texts_index,\n",
    "                                            kw='data', min_train_count=2, rel_freq_threshold=0.1)\n",
    "\n",
    "    filters = [filter_and_the, filter_stopwords_par_data, filter_intro_ssai, filler_intro_words, \n",
    "               filter_br_less_than_two_words, filter_parial_match_datasets, filter_by_train_counts_filled] \n",
    "\n",
    "    for filt in filters:\n",
    "        ssai_par_datasets = filt(ssai_par_datasets)\n",
    "    \n",
    "    ssai_par_datasets = [BR_PAT.sub('', ds) for ds in ssai_par_datasets]\n",
    "\n",
    "    return ssai_par_datasets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-28T13:09:03.420725Z",
     "iopub.status.busy": "2021-06-28T13:09:03.419933Z",
     "iopub.status.idle": "2021-06-28T13:10:45.665331Z",
     "shell.execute_reply": "2021-06-28T13:10:45.666496Z",
     "shell.execute_reply.started": "2021-06-28T13:03:40.438975Z"
    },
    "papermill": {
     "duration": 102.520519,
     "end_time": "2021-06-28T13:10:45.666917",
     "exception": false,
     "start_time": "2021-06-28T13:09:03.146398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6929257/6929257 [00:09<00:00, 737329.35it/s]\n",
      "100%|██████████| 6929257/6929257 [01:25<00:00, 80824.24it/s]\n",
      "100%|██████████| 4624/4624 [00:04<00:00, 1103.79it/s]\n",
      "4it [00:00, 11.10it/s]\n"
     ]
    }
   ],
   "source": [
    "def solution():\n",
    "    predictions = defaultdict(set)\n",
    "    datasets = get_datasets()\n",
    "    train_datasets = [ds for ds in train_labels_set if sum(ch.islower() for ch in ds) > 0 ]\n",
    "    train_datasets = [BR_PAT.sub('', ds).strip() for ds in train_labels_set]\n",
    "    datasets = set(datasets) | set(train_datasets)\n",
    "    for filename in tqdm((DATA_DIR / 'test').glob('*')):\n",
    "        idx = filename.name.split('.')[0]\n",
    "        predictions[idx]\n",
    "        with open(filename) as fin:\n",
    "            data = json.load(fin)\n",
    "        \n",
    "        for sec in data:\n",
    "            text = sec['text']    \n",
    "            current_preds = []\n",
    "            for paragraph in text.split('\\n'):\n",
    "                for sent in re.split('[\\.]', paragraph):\n",
    "                    for ds in datasets:\n",
    "                        if ds in sent:\n",
    "                            current_preds.append(ds)\n",
    "                            current_preds.extend(get_parenthesis(sent, ds))\n",
    "            predictions[idx].update(current_preds)\n",
    "        predictions[idx] = remove_substring_predictions(predictions[idx])\n",
    "\n",
    "    prediction_str_list = []\n",
    "    for idx, datasets in predictions.items():\n",
    "        datasets_str = '|'.join(clean_text(d) for d in sorted(set(datasets)))\n",
    "        prediction_str_list.append([idx, datasets_str])\n",
    "\n",
    "    with open('submission.csv', 'w') as fin:\n",
    "        for idx, datasets in [['Id', 'PredictionString']] + prediction_str_list:\n",
    "            fin.write(','.join([idx, datasets]) + '\\n')\n",
    "\n",
    "\n",
    "solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-28T13:10:46.744844Z",
     "iopub.status.busy": "2021-06-28T13:10:46.744186Z",
     "iopub.status.idle": "2021-06-28T13:10:47.523426Z",
     "shell.execute_reply": "2021-06-28T13:10:47.522827Z",
     "shell.execute_reply.started": "2021-06-28T13:05:21.830971Z"
    },
    "papermill": {
     "duration": 1.316737,
     "end_time": "2021-06-28T13:10:47.523625",
     "exception": false,
     "start_time": "2021-06-28T13:10:46.206888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id,PredictionString\r\n",
      "8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60,ces|consumer expenditure survey|national health and nutrition examination survey|ruccs|rural urban continuum codes\r\n",
      "2100032a-7c33-4bff-97ef-690822c43466,adni|alzheimer s disease neuroimaging initiative|chs|cardiovascular health study|framingham heart study\r\n",
      "2f392438-e215-4169-bebf-21ac4ff253e1,ccd|cps|current population survey|idb|international data base|nces common core of data|nces schools and staffing survey|pirls|pisa|program for international student assessment|progress in international reading literacy study|sass|timss 2007|trends in international mathematics and science study\r\n",
      "3f316b38-1a24-45a9-8d8c-4e05a42257c6,national geodetic survey|national hydrography dataset|slosh model|us geological survey|usgs\r\n"
     ]
    }
   ],
   "source": [
    "!cat submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.529706,
     "end_time": "2021-06-28T13:10:48.607702",
     "exception": false,
     "start_time": "2021-06-28T13:10:48.077996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.529328,
     "end_time": "2021-06-28T13:10:49.711505",
     "exception": false,
     "start_time": "2021-06-28T13:10:49.182177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 221.399976,
   "end_time": "2021-06-28T13:10:52.333784",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-28T13:07:10.933808",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
