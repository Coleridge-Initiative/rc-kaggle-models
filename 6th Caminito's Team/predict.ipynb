{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-06-22T13:30:44.570575Z",
     "iopub.status.busy": "2021-06-22T13:30:44.570168Z",
     "iopub.status.idle": "2021-06-22T13:30:44.579677Z",
     "shell.execute_reply": "2021-06-22T13:30:44.577382Z",
     "shell.execute_reply.started": "2021-06-22T13:30:44.570541Z"
    }
   },
   "outputs": [],
   "source": [
    "# Asthetics https://www.kaggle.com/manabendrarout/tabular-data-preparation-basic-eda-and-baseline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Basic\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from tqdm.autonotebook import tqdm\n",
    "import string\n",
    "import re\n",
    "from functools import partial\n",
    "from ipywidgets import IntProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:30:44.58341Z",
     "iopub.status.busy": "2021-06-22T13:30:44.582781Z",
     "iopub.status.idle": "2021-06-22T13:30:44.68618Z",
     "shell.execute_reply": "2021-06-22T13:30:44.685072Z",
     "shell.execute_reply.started": "2021-06-22T13:30:44.583356Z"
    }
   },
   "outputs": [],
   "source": [
    "#Upload Kaggle Data\n",
    "train_df = pd.read_csv('../../coleridgeinitiative-show-us-the-data/train.csv')\n",
    "sample_sub = pd.read_csv('../../coleridgeinitiative-show-us-the-data/sample_submission.csv')\n",
    "train_files_path = '../../coleridgeinitiative-show-us-the-data/train'\n",
    "test_files_path = '../../coleridgeinitiative-show-us-the-data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('SETTINGS.json',)\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "\n",
    "RAW_DATA_DIR = data['RAW_DATA_DIR']\n",
    "TRAIN_DATA_CLEAN_PATH = data['TRAIN_DATA_CLEAN_PATH']\n",
    "TEST_DATA_CLEAN_PATH = data['TEST_DATA_CLEAN_PATH']\n",
    "MODEL_CHECKPOINT_DIR = data['MODEL_CHECKPOINT_DIR']\n",
    "SUBMISSION_DIR = data['SUBMISSION_DIR']\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:30:44.69013Z",
     "iopub.status.busy": "2021-06-22T13:30:44.689818Z",
     "iopub.status.idle": "2021-06-22T13:30:44.706158Z",
     "shell.execute_reply": "2021-06-22T13:30:44.705116Z",
     "shell.execute_reply.started": "2021-06-22T13:30:44.6901Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>pub_title</th>\n",
       "      <th>dataset_title</th>\n",
       "      <th>dataset_label</th>\n",
       "      <th>cleaned_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d0fa7568-7d8e-4db9-870f-f9c6f668c17b</td>\n",
       "      <td>The Impact of Dual Enrollment on College Degre...</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>national education longitudinal study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2f26f645-3dec-485d-b68d-f013c9e05e60</td>\n",
       "      <td>Educational Attainment of High School Dropouts...</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>national education longitudinal study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c5d5cd2c-59de-4f29-bbb1-6a88c7b52f29</td>\n",
       "      <td>Differences in Outcomes for Female and Male St...</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>national education longitudinal study</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id  \\\n",
       "0  d0fa7568-7d8e-4db9-870f-f9c6f668c17b   \n",
       "1  2f26f645-3dec-485d-b68d-f013c9e05e60   \n",
       "2  c5d5cd2c-59de-4f29-bbb1-6a88c7b52f29   \n",
       "\n",
       "                                           pub_title  \\\n",
       "0  The Impact of Dual Enrollment on College Degre...   \n",
       "1  Educational Attainment of High School Dropouts...   \n",
       "2  Differences in Outcomes for Female and Male St...   \n",
       "\n",
       "                           dataset_title  \\\n",
       "0  National Education Longitudinal Study   \n",
       "1  National Education Longitudinal Study   \n",
       "2  National Education Longitudinal Study   \n",
       "\n",
       "                           dataset_label  \\\n",
       "0  National Education Longitudinal Study   \n",
       "1  National Education Longitudinal Study   \n",
       "2  National Education Longitudinal Study   \n",
       "\n",
       "                           cleaned_label  \n",
       "0  national education longitudinal study  \n",
       "1  national education longitudinal study  \n",
       "2  national education longitudinal study  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:30:44.708341Z",
     "iopub.status.busy": "2021-06-22T13:30:44.708004Z",
     "iopub.status.idle": "2021-06-22T13:30:44.717001Z",
     "shell.execute_reply": "2021-06-22T13:30:44.715607Z",
     "shell.execute_reply.started": "2021-06-22T13:30:44.708309Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9.!?'\"'\"'()\\[\\]]+', ' ', text)\n",
    "    text = re.sub(\"'\", '', text)\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = re.sub('[.]{2,}', '.', text)\n",
    "    text = re.sub(r'\\. \\.', '.', text)\n",
    "    text = re.sub(r' \\.', '.', text)    \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:30:44.719443Z",
     "iopub.status.busy": "2021-06-22T13:30:44.718944Z",
     "iopub.status.idle": "2021-06-22T13:30:44.735122Z",
     "shell.execute_reply": "2021-06-22T13:30:44.733795Z",
     "shell.execute_reply.started": "2021-06-22T13:30:44.719399Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_append_return(filename, train_files_path=train_files_path, output='text'):\n",
    "    json_path = os.path.join(train_files_path, (filename+'.json'))\n",
    "    headings = []\n",
    "    contents = []\n",
    "    combined = []\n",
    "    with open(json_path, 'r') as f: #encoding='utf-8'\n",
    "        json_decode = json.load(f)\n",
    "        for data in json_decode:\n",
    "            headings.append(data.get('section_title'))\n",
    "            s = text_cleaning(data.get('text'))\n",
    "            if len(s) > 200000:\n",
    "                #s = data.get('text')\n",
    "                #print(data.get('text'))\n",
    "                l = s.split()\n",
    "                n = 100000\n",
    "                texto = [\" \".join(l[x:x+n]) for x in range(0, len(l), n)]   \n",
    "                contents.extend(texto)\n",
    "            else:\n",
    "                contents.append(s)\n",
    "                #print(contents1)\n",
    "            combined.append(data.get('section_title'))\n",
    "            combined.append(data.get('text'))\n",
    "    \n",
    "    all_headings = ' '.join(headings)\n",
    "    all_contents = ' '.join(contents)\n",
    "    #contents = contents1.extend(contents2)\n",
    "    #print(combined)\n",
    "    all_data = '. '.join(combined)\n",
    "    \n",
    "    if output == 'text':\n",
    "        return all_contents\n",
    "    elif output == 'head':\n",
    "        return all_headings\n",
    "    elif output == 'comb':\n",
    "        return contents\n",
    "    else:\n",
    "        #print(\"FEDE\")\n",
    "        return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:30:44.737513Z",
     "iopub.status.busy": "2021-06-22T13:30:44.737075Z",
     "iopub.status.idle": "2021-06-22T13:30:44.748298Z",
     "shell.execute_reply": "2021-06-22T13:30:44.746995Z",
     "shell.execute_reply.started": "2021-06-22T13:30:44.737477Z"
    }
   },
   "outputs": [],
   "source": [
    "#tqdm.pandas()\n",
    "#train_df['text_all'] = train_df['Id'].progress_apply(partial(read_append_return, output='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:30:44.750624Z",
     "iopub.status.busy": "2021-06-22T13:30:44.75019Z",
     "iopub.status.idle": "2021-06-22T13:30:44.765471Z",
     "shell.execute_reply": "2021-06-22T13:30:44.764304Z",
     "shell.execute_reply.started": "2021-06-22T13:30:44.75056Z"
    }
   },
   "outputs": [],
   "source": [
    "#tqdm.pandas()\n",
    "#train_df['text_all'] = train_df['text_all'].progress_apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:30:44.768096Z",
     "iopub.status.busy": "2021-06-22T13:30:44.767777Z",
     "iopub.status.idle": "2021-06-22T13:30:44.884855Z",
     "shell.execute_reply": "2021-06-22T13:30:44.883662Z",
     "shell.execute_reply.started": "2021-06-22T13:30:44.768066Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efce03bff8cd40e49bcf287611b0765d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "sample_sub['text_all'] = sample_sub['Id'].progress_apply(partial(read_append_return, train_files_path=test_files_path, output='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:30:44.88692Z",
     "iopub.status.busy": "2021-06-22T13:30:44.886614Z",
     "iopub.status.idle": "2021-06-22T13:30:45.003076Z",
     "shell.execute_reply": "2021-06-22T13:30:45.002036Z",
     "shell.execute_reply.started": "2021-06-22T13:30:44.886889Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71362b047d274dd3bb36ae26431a42ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "sample_sub['text_all'] = sample_sub['text_all'].progress_apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:30:45.00535Z",
     "iopub.status.busy": "2021-06-22T13:30:45.004899Z",
     "iopub.status.idle": "2021-06-22T13:30:45.019907Z",
     "shell.execute_reply": "2021-06-22T13:30:45.018884Z",
     "shell.execute_reply.started": "2021-06-22T13:30:45.005303Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PredictionString</th>\n",
       "      <th>text_all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abstract. Cognitive deficits and reduced educa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2f392438-e215-4169-bebf-21ac4ff253e1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Introduction. This report describes how the ed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3f316b38-1a24-45a9-8d8c-4e05a42257c6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INTRODUCTION. Cape Hatteras National Seashore ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Introduction. A significant body of research h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id  PredictionString  \\\n",
       "0  2100032a-7c33-4bff-97ef-690822c43466               NaN   \n",
       "1  2f392438-e215-4169-bebf-21ac4ff253e1               NaN   \n",
       "2  3f316b38-1a24-45a9-8d8c-4e05a42257c6               NaN   \n",
       "3  8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60               NaN   \n",
       "\n",
       "                                            text_all  \n",
       "0  Abstract. Cognitive deficits and reduced educa...  \n",
       "1  Introduction. This report describes how the ed...  \n",
       "2  INTRODUCTION. Cape Hatteras National Seashore ...  \n",
       "3  Introduction. A significant body of research h...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:30:45.02209Z",
     "iopub.status.busy": "2021-06-22T13:30:45.021789Z",
     "iopub.status.idle": "2021-06-22T13:30:45.027787Z",
     "shell.execute_reply": "2021-06-22T13:30:45.026524Z",
     "shell.execute_reply.started": "2021-06-22T13:30:45.022062Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:30:45.029312Z",
     "iopub.status.busy": "2021-06-22T13:30:45.028889Z",
     "iopub.status.idle": "2021-06-22T13:30:45.04072Z",
     "shell.execute_reply": "2021-06-22T13:30:45.039927Z",
     "shell.execute_reply.started": "2021-06-22T13:30:45.02928Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from typing import Tuple, List, Optional, Set, Dict\n",
    "from collections import defaultdict\n",
    "from spacy.tokens import Span, Doc\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.pipeline import EntityRuler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:30:45.042293Z",
     "iopub.status.busy": "2021-06-22T13:30:45.041871Z",
     "iopub.status.idle": "2021-06-22T13:30:45.076751Z",
     "shell.execute_reply": "2021-06-22T13:30:45.075919Z",
     "shell.execute_reply.started": "2021-06-22T13:30:45.042264Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_abbreviation(\n",
    "    long_form_candidate: Span, short_form_candidate: Span\n",
    ") -> Tuple[Span, Optional[Span]]:\n",
    "    \"\"\"\n",
    "    Implements the abbreviation detection algorithm in \"A simple algorithm\n",
    "    for identifying abbreviation definitions in biomedical text.\", (Schwartz & Hearst, 2003).\n",
    "\n",
    "    The algorithm works by enumerating the characters in the short form of the abbreviation,\n",
    "    checking that they can be matched against characters in a candidate text for the long form\n",
    "    in order, as well as requiring that the first letter of the abbreviated form matches the\n",
    "    _beginning_ letter of a word.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    long_form_candidate: Span, required.\n",
    "        The spaCy span for the long form candidate of the definition.\n",
    "    short_form_candidate: Span, required.\n",
    "        The spaCy span for the abbreviation candidate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A Tuple[Span, Optional[Span]], representing the short form abbreviation and the\n",
    "    span corresponding to the long form expansion, or None if a match is not found.\n",
    "    \"\"\"\n",
    "    long_form = \" \".join([x.text for x in long_form_candidate])\n",
    "    short_form = \" \".join([x.text for x in short_form_candidate])\n",
    "\n",
    "    long_index = len(long_form) - 1\n",
    "    short_index = len(short_form) - 1\n",
    "\n",
    "    while short_index >= 0:\n",
    "        current_char = short_form[short_index].lower()\n",
    "        # We don't check non alpha-numeric characters.\n",
    "        if not current_char.isalnum():\n",
    "            short_index -= 1\n",
    "            continue\n",
    "\n",
    "            # Does the character match at this position? ...\n",
    "        while (\n",
    "            (long_index >= 0 and long_form[long_index].lower() != current_char)\n",
    "            or\n",
    "            # .... or if we are checking the first character of the abbreviation, we enforce\n",
    "            # to be the _starting_ character of a span.\n",
    "            (\n",
    "                short_index == 0\n",
    "                and long_index > 0\n",
    "                and long_form[long_index - 1].isalnum()\n",
    "            )\n",
    "        ):\n",
    "            long_index -= 1\n",
    "\n",
    "        if long_index < 0:\n",
    "            return short_form_candidate, None\n",
    "\n",
    "        long_index -= 1\n",
    "        short_index -= 1\n",
    "\n",
    "    # The last subtraction will either take us on to a whitespace character, or\n",
    "    # off the front of the string (i.e. long_index == -1). Either way, we want to add\n",
    "    # one to get back to the start character of the long form\n",
    "    long_index += 1\n",
    "\n",
    "    # Now we know the character index of the start of the character span,\n",
    "    # here we just translate that to the first token beginning after that\n",
    "    # value, so we can return a spaCy span instead.\n",
    "    word_lengths = 0\n",
    "    starting_index = None\n",
    "    for i, word in enumerate(long_form_candidate):\n",
    "        # need to add 1 for the space characters\n",
    "        word_lengths += len(word.text_with_ws)\n",
    "        if word_lengths > long_index:\n",
    "            starting_index = i\n",
    "            break\n",
    "\n",
    "    return short_form_candidate, long_form_candidate[starting_index:]\n",
    "\n",
    "def filter_matches(\n",
    "    matcher_output: List[Tuple[int, int, int]], doc: Doc\n",
    ") -> List[Tuple[Span, Span]]:\n",
    "    # Filter into two cases:\n",
    "    # 1. <Short Form> ( <Long Form> )\n",
    "    # 2. <Long Form> (<Short Form>) [this case is most common].\n",
    "    candidates = []\n",
    "    for match in matcher_output:\n",
    "        start = match[1]\n",
    "        end = match[2]\n",
    "        # Ignore spans with more than 8 words in them, and spans at the start of the doc\n",
    "        if end - start > 8 or start == 1:\n",
    "            continue\n",
    "        if end - start > 3:\n",
    "            # Long form is inside the parens.\n",
    "            # Take one word before.\n",
    "            short_form_candidate = doc[start - 2 : start - 1]\n",
    "            long_form_candidate = doc[start:end]\n",
    "        else:\n",
    "            # Normal case.\n",
    "            # Short form is inside the parens.\n",
    "            short_form_candidate = doc[start:end]\n",
    "\n",
    "            # Sum character lengths of contents of parens.\n",
    "            abbreviation_length = sum([len(x) for x in short_form_candidate])\n",
    "            max_words = min(abbreviation_length + 5, abbreviation_length * 2)\n",
    "            # Look up to max_words backwards\n",
    "            long_form_candidate = doc[max(start - max_words - 1, 0) : start - 1]\n",
    "\n",
    "        # add candidate to candidates if candidates pass filters\n",
    "        if short_form_filter(short_form_candidate):\n",
    "            candidates.append((long_form_candidate, short_form_candidate))\n",
    "\n",
    "    return candidates\n",
    "\n",
    "def short_form_filter(span: Span) -> bool:\n",
    "    # All words are between length 2 and 10\n",
    "    if not all([2 <= len(x) < 10 for x in span]):\n",
    "        return False\n",
    "\n",
    "    # At least 50% of the short form should be alpha\n",
    "    if (sum([c.isalpha() for c in span.text]) / len(span.text)) < 0.5:\n",
    "        return False\n",
    "\n",
    "    # The first character of the short form should be alpha\n",
    "    if not span.text[0].isalpha():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "class AbbreviationDetector:\n",
    "    \"\"\"\n",
    "    Detects abbreviations using the algorithm in \"A simple algorithm for identifying\n",
    "    abbreviation definitions in biomedical text.\", (Schwartz & Hearst, 2003).\n",
    "\n",
    "    This class sets the `._.abbreviations` attribute on spaCy Doc.\n",
    "\n",
    "    The abbreviations attribute is a `List[Span]` where each Span has the `Span._.long_form`\n",
    "    attribute set to the long form definition of the abbreviation.\n",
    "\n",
    "    Note that this class does not replace the spans, or merge them.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nlp) -> None:\n",
    "        Doc.set_extension(\"abbreviations\", default=[], force=True)\n",
    "        Span.set_extension(\"long_form\", default=None, force=True)\n",
    "\n",
    "        self.matcher = Matcher(nlp.vocab)\n",
    "        self.matcher.add(\n",
    "            \"parenthesis\", None, [{\"ORTH\": \"(\"}, {\"OP\": \"+\"}, {\"ORTH\": \")\"}]\n",
    "        )\n",
    "        self.global_matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    def find(self, span: Span, doc: Doc) -> Tuple[Span, Set[Span]]:\n",
    "        \"\"\"\n",
    "        Functional version of calling the matcher for a single span.\n",
    "        This method is helpful if you already have an abbreviation which\n",
    "        you want to find a definition for.\n",
    "        \"\"\"\n",
    "        dummy_matches = [(-1, int(span.start), int(span.end))]\n",
    "        filtered = filter_matches(dummy_matches, doc)\n",
    "        abbreviations = self.find_matches_for(filtered, doc)\n",
    "\n",
    "        if not abbreviations:\n",
    "            return span, set()\n",
    "        else:\n",
    "            return abbreviations[0]\n",
    "    def __call__(self, doc: Doc) -> Doc:\n",
    "        matches = self.matcher(doc)\n",
    "        matches_no_brackets = [(x[0], x[1] + 1, x[2] - 1) for x in matches]\n",
    "        filtered = filter_matches(matches_no_brackets, doc)\n",
    "        occurences = self.find_matches_for(filtered, doc)\n",
    "\n",
    "        for (long_form, short_forms) in occurences:\n",
    "            for short in short_forms:\n",
    "                short._.long_form = long_form\n",
    "                doc._.abbreviations.append(short)\n",
    "        return doc\n",
    "\n",
    "    def find_matches_for(\n",
    "        self, filtered: List[Tuple[Span, Span]], doc: Doc\n",
    "    ) -> List[Tuple[Span, Set[Span]]]:\n",
    "        rules = {}\n",
    "        all_occurences: Dict[Span, Set[Span]] = defaultdict(set)\n",
    "        already_seen_long: Set[str] = set()\n",
    "        already_seen_short: Set[str] = set()\n",
    "        for (long_candidate, short_candidate) in filtered:\n",
    "            short, long = find_abbreviation(long_candidate, short_candidate)\n",
    "            # We need the long and short form definitions to be unique, because we need\n",
    "            # to store them so we can look them up later. This is a bit of a\n",
    "            # pathalogical case also, as it would mean an abbreviation had been\n",
    "            # defined twice in a document. There's not much we can do about this,\n",
    "            # but at least the case which is discarded will be picked up below by\n",
    "            # the global matcher. So it's likely that things will work out ok most of the time.\n",
    "            new_long = long.string not in already_seen_long if long else False\n",
    "            new_short = short.string not in already_seen_short\n",
    "            if long is not None and new_long and new_short:\n",
    "                already_seen_long.add(long.string)\n",
    "                already_seen_short.add(short.string)\n",
    "                all_occurences[long].add(short)\n",
    "                rules[long.string] = long\n",
    "                # Add a rule to a matcher to find exactly this substring.\n",
    "                self.global_matcher.add(\n",
    "                    long.string, None, [{\"ORTH\": x.text} for x in short]\n",
    "                )\n",
    "        to_remove = set()\n",
    "        global_matches = self.global_matcher(doc)\n",
    "        for match, start, end in global_matches:\n",
    "            string_key = self.global_matcher.vocab.strings[match]\n",
    "            to_remove.add(string_key)\n",
    "            all_occurences[rules[string_key]].add(doc[start:end])\n",
    "        for key in to_remove:\n",
    "            # Clean up the global matcher.\n",
    "            self.global_matcher.remove(key)\n",
    "        return list((k, v) for k, v in all_occurences.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:30:45.079587Z",
     "iopub.status.busy": "2021-06-22T13:30:45.078895Z",
     "iopub.status.idle": "2021-06-22T13:30:55.759191Z",
     "shell.execute_reply": "2021-06-22T13:30:55.758205Z",
     "shell.execute_reply.started": "2021-06-22T13:30:45.079534Z"
    }
   },
   "outputs": [],
   "source": [
    "#This model is a previous NER-LAST-VEC-1100-RULER model, but has the same entity-ruler and vectors.\n",
    "#That is why we use it for string matching and vectors similarity.\n",
    "\n",
    "nlp = spacy.load(MODEL_CHECKPOINT_DIR + \"NER-LAST-VEC-716-RULER\", disable = ['tagger', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:30:55.761183Z",
     "iopub.status.busy": "2021-06-22T13:30:55.760721Z",
     "iopub.status.idle": "2021-06-22T13:30:55.767548Z",
     "shell.execute_reply": "2021-06-22T13:30:55.766624Z",
     "shell.execute_reply.started": "2021-06-22T13:30:55.761128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['parser', 'entity_ruler', 'AbbreviationDetector']\n"
     ]
    }
   ],
   "source": [
    "# Add the abbreviation pipe to the spacy pipeline.\n",
    "abbreviation_pipe = AbbreviationDetector(nlp)\n",
    "nlp.add_pipe(abbreviation_pipe)\n",
    "Language.factories['AbbreviationDetector'] = lambda nlp, **cfg: AbbreviationDetector(nlp, **cfg)\n",
    "nlp.max_length = 15000000\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:30:55.769449Z",
     "iopub.status.busy": "2021-06-22T13:30:55.768863Z",
     "iopub.status.idle": "2021-06-22T13:30:55.822097Z",
     "shell.execute_reply": "2021-06-22T13:30:55.821107Z",
     "shell.execute_reply.started": "2021-06-22T13:30:55.769402Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1039"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_org_df = pd.read_csv(RAW_DATA_DIR + 'International_ORG_sin_duplicados.csv')\n",
    "list3 = int_org_df['Text_Org'].tolist()\n",
    "\n",
    "to_vec = pd.read_csv(RAW_DATA_DIR + 'cleanedLabel_toVec_Diego_for_vectors.csv')\n",
    "list2 = [text_cleaning(x).lower().rstrip() for x in to_vec['0'].unique()]\n",
    "\n",
    "list2 = set(list2)\n",
    "len(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:30:55.823955Z",
     "iopub.status.busy": "2021-06-22T13:30:55.823444Z",
     "iopub.status.idle": "2021-06-22T13:31:07.644793Z",
     "shell.execute_reply": "2021-06-22T13:31:07.64354Z",
     "shell.execute_reply.started": "2021-06-22T13:30:55.823902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['textcat']\n"
     ]
    }
   ],
   "source": [
    "nlp2 = spacy.load(MODEL_CHECKPOINT_DIR + 'Ensemble_30000_textcat')\n",
    "nlp3 = spacy.load(MODEL_CHECKPOINT_DIR + 'NER-LAST-VEC-1100-RULER', disable = ['tagger', 'parser'])\n",
    "nlp4 = spacy.load(MODEL_CHECKPOINT_DIR + 'Ensemble_3000_dataset_vs_ORG')\n",
    "\n",
    "print(nlp4.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:31:07.646258Z",
     "iopub.status.busy": "2021-06-22T13:31:07.645954Z",
     "iopub.status.idle": "2021-06-22T13:31:11.678019Z",
     "shell.execute_reply": "2021-06-22T13:31:11.676995Z",
     "shell.execute_reply.started": "2021-06-22T13:31:07.64623Z"
    }
   },
   "outputs": [],
   "source": [
    "#Vectors\n",
    "list2SpacyDocs = [nlp(x) for x in list2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:31:11.680547Z",
     "iopub.status.busy": "2021-06-22T13:31:11.680256Z",
     "iopub.status.idle": "2021-06-22T13:31:11.68416Z",
     "shell.execute_reply": "2021-06-22T13:31:11.683026Z",
     "shell.execute_reply.started": "2021-06-22T13:31:11.680519Z"
    }
   },
   "outputs": [],
   "source": [
    "#train_df_unique = train_df.drop_duplicates(subset=['Id'])\n",
    "#print (len(train_df_unique['Id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f30149c8b44b64912767f96a8e8dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-44170d126aa9>:105: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  similarity2 = text1.similarity(vec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'charge', 'cohorts for heart and aging research in genomic epidemiology charge', 'framingham heart study second generation cohort fhs', 'pediatric imaging neurocognition and genetics ping', 'ping', 'cardiovascular health study chs', 'alzheimers disease neuroimaging initiative adni', 'chs', 'adni', 'fhs'}\n",
      "Sentences classified as positive: 9\n",
      "Rul-Ents classified as positive: 12\n",
      "{'sass', 'cps', 'integrated postsecondary education data system ipeds', 'timss', 'pirls', 'progress in international reading literacy study pirls', 'pisa', 'program for international student assessment pisa', 'common core of data ccd', 'ccd', 'schools and staffing survey sass', 'current population survey cps', 'trends in international mathematics and science study timss', 'ipeds'}\n",
      "Sentences classified as positive: 102\n",
      "Rul-Ents classified as positive: 131\n",
      "{'noaa storm surge inundation', 'sea lake and overland surges from hurricanes slosh', 'geographic information systems gis', 'national hydrography dataset', 'slosh', 'gis', 'maximum envelope of water meow', 'slrrms', 'sea level rise risk management study', 'national geodetic survey', 'meow', 'sea level rise risk management study slrrms', 'coastal erosion study'}\n",
      "Sentences classified as positive: 61\n",
      "Rul-Ents classified as positive: 35\n",
      "{'ruccs', 'national health and nutrition examination survey', 'consumer expenditure survey', 'rural urban continuum codes ruccs', 'ces'}\n",
      "Sentences classified as positive: 12\n",
      "Rul-Ents classified as positive: 2\n"
     ]
    }
   ],
   "source": [
    "#Abvr-NER-Sentences Combino 2 modelos nlp\n",
    "id_list = []\n",
    "lables_list = []\n",
    "threshold = 0.90\n",
    "threshold_data = 0.95\n",
    "ne = 0\n",
    "maxlen=1000000\n",
    "\n",
    "for index, row in tqdm(sample_sub.iterrows()):\n",
    "    llabels = []\n",
    "    papertxt = row['text_all']\n",
    "    row_id = row['Id']\n",
    "    nlp_labels = []\n",
    "    docs = []\n",
    "    if len(papertxt) > maxlen:\n",
    "        i = round(len(papertxt)/maxlen)-1\n",
    "        while i >= 0:\n",
    "            docs.append(papertxt[0:maxlen-1])\n",
    "            papertxt = papertxt[maxlen:]\n",
    "            i = i - 1\n",
    "    else:\n",
    "        docs.append(papertxt)\n",
    "        \n",
    "    abvr_list = []\n",
    "    if docs:\n",
    "        for text in docs:\n",
    "            doc = nlp(text)\n",
    "            for abrv in doc._.abbreviations:\n",
    "                if any(abrv.text in s for s in abvr_list):\n",
    "                    pass\n",
    "                else:\n",
    "                    if abrv._.long_form.text not in list3 and abrv.text != \"AD\" and not abrv.text.islower() and not abrv._.long_form.text.islower(): \n",
    "                        #print(abrv.text)\n",
    "                        long_data = abrv._.long_form.text + \", \" + abrv.text\n",
    "                        if abvr_list:\n",
    "                            if any(abrv.text in s for s in abvr_list):\n",
    "                                pass\n",
    "                            else:\n",
    "                                abvr_list.append(long_data)                           \n",
    "                        else:\n",
    "                            abvr_list.append(long_data)\n",
    "                            \n",
    "            #print(abvr_list)\n",
    "            en = 0\n",
    "            for ent in doc.ents:\n",
    "                en += 1\n",
    "                if ent.label_ == \"RULDATA\":                    \n",
    "                    #print(\"In list: \", ent.text)\n",
    "                    if any(ent.text.lower() in s.lower() for s in abvr_list):\n",
    "                        for words in abvr_list:\n",
    "                            palabras = words.split(\", \")\n",
    "                            #print(palabras[0])\n",
    "                            abrv_word = palabras[-1]\n",
    "                            #print(abrv_word)\n",
    "                            if abrv_word in ent.text or palabras[0] in ent.text:\n",
    "                                #print(ent.text)\n",
    "                                nlp_labels.append(words)\n",
    "                                nlp_labels.append(abrv_word)                             \n",
    "                    else:                        \n",
    "                        if len(ent) > 2:\n",
    "                            long_ent = ent.text\n",
    "                            long_ent_find = re.findall('\\(([^)]+)', long_ent)\n",
    "                            if long_ent_find:\n",
    "                                abrv_ent = re.search('\\(([^)]+)', long_ent).group(1)                        \n",
    "                                nlp_labels.append(long_ent)\n",
    "                                nlp_labels.append(abrv_ent)\n",
    "                            else:\n",
    "                                nlp_labels.append(long_ent)\n",
    "            #print(abvr_list)\n",
    "            #Vectores\n",
    "            labels_vec_set = set(nlp_labels)\n",
    "            #print(labels_vec_set)\n",
    "            label_vec = [nlp(x.lower()) for x in labels_vec_set]\n",
    "            np = 0\n",
    "            for sent in doc.sents:\n",
    "                slen = len(sent.text.split())\n",
    "                if slen >= 7 and slen <= 100:\n",
    "                    strsent = str(sent.text)\n",
    "                    strsent = strsent.strip()\n",
    "                    doccat = nlp2(strsent)\n",
    "                    if doccat.cats['SI'] > threshold:\n",
    "                        np += 1\n",
    "                        docent = nlp3(strsent)\n",
    "                        for ent in docent.ents:                            \n",
    "                            if any(ent.text.lower() in s.lower() for s in abvr_list) and not any(ent.text.lower() in s.lower() for s in labels_vec_set):\n",
    "                                #print(\"In list: \", ent.text)\n",
    "                                for s in abvr_list:                                    \n",
    "                                    if ent.text.lower() in s.lower():                                            \n",
    "                                        palabras = s.split(\", \")\n",
    "                                        data_vs_org = nlp4(palabras[0].lower())                                \n",
    "                                        #print(ent.text)\n",
    "                                        if data_vs_org.cats['SI'] > threshold_data:                                            \n",
    "                                            nlp_labels.append(palabras[0])\n",
    "                                            nlp_labels.append(palabras[-1]) \n",
    "                                            \n",
    "                            else:\n",
    "                                paso = False\n",
    "                                text1 = nlp(ent.text.lower())                                \n",
    "                                data_vs_org = nlp4(ent.text)                                \n",
    "                                #print(ent.text)\n",
    "                                if data_vs_org.cats['SI'] > threshold_data:\n",
    "                                    paso = True                                    \n",
    "                                    for vec in label_vec:\n",
    "                                        #print(\"Lista ruler: \", vec)\n",
    "                                        similarity2 = text1.similarity(vec)\n",
    "                                        #print(similarity2)\n",
    "                                        #print(\"Lista ner: \", text1)\n",
    "                                        #print(\"........\")                                        \n",
    "                                        if similarity2 > 0.90:\n",
    "                                            #print(ent.text)\n",
    "                                            paso = False\n",
    "                                if paso:\n",
    "                                    #print(ent.text)\n",
    "                                    nlp_labels.append(ent.text)                \n",
    "            \n",
    "    cleaned_labels = [clean_text(x) for x in nlp_labels]\n",
    "    #cleaned_labels =[]\n",
    "    cleaned_labels = set(cleaned_labels)\n",
    "    lables_list.append('|'.join(cleaned_labels))\n",
    "    id_list.append(str(row_id))\n",
    "    #print(\"LABELS\")\n",
    "    print(cleaned_labels)\n",
    "    print('Sentences classified as positive:', np)\n",
    "    print('Rul-Ents classified as positive:', en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:31:41.031385Z",
     "iopub.status.busy": "2021-06-22T13:31:41.03084Z",
     "iopub.status.idle": "2021-06-22T13:31:41.047358Z",
     "shell.execute_reply": "2021-06-22T13:31:41.046145Z",
     "shell.execute_reply.started": "2021-06-22T13:31:41.031324Z"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['Id'] = id_list\n",
    "submission['PredictionString'] = lables_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:31:41.049082Z",
     "iopub.status.busy": "2021-06-22T13:31:41.048738Z",
     "iopub.status.idle": "2021-06-22T13:31:41.899269Z",
     "shell.execute_reply": "2021-06-22T13:31:41.897933Z",
     "shell.execute_reply.started": "2021-06-22T13:31:41.04905Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id,PredictionString\r\n",
      "2100032a-7c33-4bff-97ef-690822c43466,charge|cohorts for heart and aging research in genomic epidemiology charge|framingham heart study second generation cohort fhs|pediatric imaging neurocognition and genetics ping|ping|cardiovascular health study chs|alzheimers disease neuroimaging initiative adni|chs|adni|fhs\r\n",
      "2f392438-e215-4169-bebf-21ac4ff253e1,sass|cps|integrated postsecondary education data system ipeds|timss|pirls|progress in international reading literacy study pirls|pisa|program for international student assessment pisa|common core of data ccd|ccd|schools and staffing survey sass|current population survey cps|trends in international mathematics and science study timss|ipeds\r\n",
      "3f316b38-1a24-45a9-8d8c-4e05a42257c6,noaa storm surge inundation|sea lake and overland surges from hurricanes slosh|geographic information systems gis|national hydrography dataset|slosh|gis|maximum envelope of water meow|slrrms|sea level rise risk management study|national geodetic survey|meow|sea level rise risk management study slrrms|coastal erosion study\r\n",
      "8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60,ruccs|national health and nutrition examination survey|consumer expenditure survey|rural urban continuum codes ruccs|ces\r\n"
     ]
    }
   ],
   "source": [
    "submission.to_csv(SUBMISSION_DIR + 'submission.csv', index=False)\n",
    "!head 'submissions/submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T15:43:28.067857Z",
     "iopub.status.busy": "2021-07-04T15:43:28.067479Z",
     "iopub.status.idle": "2021-07-04T15:43:35.032467Z",
     "shell.execute_reply": "2021-07-04T15:43:35.031182Z",
     "shell.execute_reply.started": "2021-07-04T15:43:28.067822Z"
    }
   },
   "outputs": [],
   "source": [
    "#!find . -type d > directory_structure.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
