{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49f1f31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asthetics https://www.kaggle.com/manabendrarout/tabular-data-preparation-basic-eda-and-baseline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Basic\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from tqdm.autonotebook import tqdm\n",
    "import string\n",
    "import re\n",
    "from functools import partial\n",
    "from ipywidgets import IntProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8b91c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload Kaggle Data\n",
    "train_df = pd.read_csv('../../coleridgeinitiative-show-us-the-data/train.csv')\n",
    "sample_sub = pd.read_csv('../../coleridgeinitiative-show-us-the-data/sample_submission.csv')\n",
    "train_files_path = '../../coleridgeinitiative-show-us-the-data/train'\n",
    "test_files_path = '../../coleridgeinitiative-show-us-the-data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9dc613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('SETTINGS.json',)\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "\n",
    "RAW_DATA_DIR = data['RAW_DATA_DIR']\n",
    "TRAIN_DATA_CLEAN_PATH = data['TRAIN_DATA_CLEAN_PATH']\n",
    "TEST_DATA_CLEAN_PATH = data['TEST_DATA_CLEAN_PATH']\n",
    "MODEL_CHECKPOINT_DIR = data['MODEL_CHECKPOINT_DIR']\n",
    "SUBMISSION_DIR = data['SUBMISSION_DIR']\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb97c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    '''\n",
    "    Removes special charecters, multiple spaces\n",
    "    text - Sentence that needs to be cleaned\n",
    "    '''\n",
    "    text = re.sub(r'[^A-Za-z0-9.!?'\"'\"'()\\[\\]]+', ' ', text)\n",
    "    text = re.sub(\"'\", '', text)\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = re.sub('[.]{2,}', '.', text)\n",
    "    text = re.sub(r'\\. \\.', '.', text)\n",
    "    text = re.sub(r' \\.', '.', text)    \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45ee1bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_append_return(filename, train_files_path=train_files_path, output='text'):\n",
    "    json_path = os.path.join(train_files_path, (filename+'.json'))\n",
    "    headings = []\n",
    "    contents = []\n",
    "    combined = []\n",
    "    with open(json_path, 'r') as f: #encoding='utf-8'\n",
    "        json_decode = json.load(f)\n",
    "        for data in json_decode:\n",
    "            headings.append(data.get('section_title'))\n",
    "            s = text_cleaning(data.get('text'))\n",
    "            if len(s) > 200000:\n",
    "                #s = data.get('text')\n",
    "                #print(data.get('text'))\n",
    "                l = s.split()\n",
    "                n = 100000\n",
    "                texto = [\" \".join(l[x:x+n]) for x in range(0, len(l), n)]   \n",
    "                contents.extend(texto)\n",
    "            else:\n",
    "                contents.append(s)\n",
    "                #print(contents1)\n",
    "            combined.append(data.get('section_title'))\n",
    "            combined.append(data.get('text'))\n",
    "    \n",
    "    all_headings = ' '.join(headings)\n",
    "    all_contents = ' '.join(contents)\n",
    "    #contents = contents1.extend(contents2)\n",
    "    #print(combined)\n",
    "    all_data = '. '.join(combined)\n",
    "    \n",
    "    if output == 'text':\n",
    "        return all_contents\n",
    "    elif output == 'head':\n",
    "        return all_headings\n",
    "    elif output == 'comb':\n",
    "        return contents\n",
    "    else:\n",
    "        return all_data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87e8fafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e72005cc5d41cb907f79462e470b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19661 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "train_df['text_all'] = train_df['Id'].progress_apply(partial(read_append_return, output='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be924bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5e6eb493cf4c61be16274bf841d9c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19661 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "train_df['text_all'] = train_df['text_all'].progress_apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be40f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_vec = pd.read_csv(RAW_DATA_DIR + 'cleanedLabel_toVec_Diego_for_vectors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3549457c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1189\n"
     ]
    }
   ],
   "source": [
    "#Prepare Dataset list\n",
    "temp_1 = [text_cleaning(x) for x in train_df['dataset_label'].unique()]\n",
    "temp_2 = [text_cleaning(x) for x in train_df['dataset_title'].unique()]\n",
    "temp_3 = [text_cleaning(x) for x in train_df['cleaned_label'].unique()]\n",
    "temp_4 = [text_cleaning(x) for x in to_vec['0'].unique()]\n",
    "\n",
    "existing_labels = set(temp_1 + temp_2 + temp_3 + temp_4)\n",
    "\n",
    "print(len(existing_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e264c54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['parser', 'AbbreviationDetector']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "nlp = spacy.load(\"en_core_sci_sm\", disable = ['ner', 'tagger', 'attribute_ruler', 'lemmatizer', 'tok2vec'])\n",
    "nlp.max_length = 10000000\n",
    "abbreviation_pipe = AbbreviationDetector(nlp)\n",
    "nlp.add_pipe(abbreviation_pipe)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc4fb2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prepare spaCy entity-ruler(For sentences that have the word dataset or a synonym.)\n",
    "syn_dataset = ['dataset','datasets', 'data-set', 'data-sets', 'data sets', 'data set', 'datum', 'databases', 'database', 'data bank', 'data banks', 'databank', 'databanks', 'metadata', 'raw data', 'time series', 'time-series']\n",
    "patterns = []\n",
    "\n",
    "for dataset in syn_dataset:\n",
    "    phrase = []\n",
    "    for word in nlp(dataset):\n",
    "        pattern = {}\n",
    "        pattern[\"LOWER\"] = str(word)\n",
    "        phrase.append(pattern)\n",
    "    #patterns.append({\"label\": dataset, \"pattern\": phrase})\n",
    "    patterns.append({\"label\": \"DATASET\", \"pattern\": phrase})\n",
    "\n",
    "len(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74b646ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['parser', 'AbbreviationDetector', 'entity_ruler']\n"
     ]
    }
   ],
   "source": [
    "from spacy.pipeline import EntityRuler\n",
    "ruler = EntityRuler(nlp, overwrite_ents=True)\n",
    "nlp.add_pipe(ruler)\n",
    "ruler.add_patterns(patterns)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "611bffe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14316\n"
     ]
    }
   ],
   "source": [
    "#Drop duplicates papers\n",
    "train_df_unique = train_df.drop_duplicates(subset=['Id'])\n",
    "print (len(train_df_unique['Id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2b78df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload organizations list\n",
    "int_org_df = pd.read_csv(RAW_DATA_DIR + 'International_ORG_sin_duplicados.csv')\n",
    "list3 = int_org_df['Text_Org'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f6a0a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1189"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List of dataset\n",
    "list2 = existing_labels\n",
    "list2_to_l = list(list2)\n",
    "len(list2_to_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3a38763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7a0c30e6254193b39577c7536fe54e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Child Left Behind (NCLB)\n",
      "National Education Longitudinal Study of 1988 (NELS 88)\n",
      "National Education Longitudinal Study (NELS)\n",
      "National Education Longitudinal Study (NELS)\n",
      "National Longitudinal Transition Study (NLTS)\n",
      "No Child Left Behind (NCLB)\n",
      "National Education Longitudinal Study of 1988 (NELS 88)\n",
      "National Education Longitudinal Study of 1988 (NELS 88)\n",
      "National Education Longitudinal Study (NELS)\n",
      "National Education Longitudinal Study (NELS)\n",
      "National Longitudinal Transition Study (NLTS)\n",
      "No Child Left Behind (NCLB)\n",
      "National Education Longitudinal Study of 1988 (NELS 88)\n",
      "National Education Longitudinal Study (NELS)\n",
      "National Longitudinal Transition Study (NLTS)\n",
      "No Child Left Behind (NCLB)\n",
      "National Education Longitudinal Study of 1988 (NELS 88)\n",
      "National Education Longitudinal Study (NELS)\n",
      "National Longitudinal Transition Study (NLTS)\n",
      "No Child Left Behind (NCLB)\n",
      "National Education Longitudinal Study of 1988 (NELS 88)\n",
      "National Education Longitudinal Study (NELS)\n",
      "National Longitudinal Transition Study (NLTS)\n",
      "National Education Longitudinal Study of 1988 (NELS 88)\n",
      "National Longitudinal Survey of the Youth (NLSY)\n",
      "National Longitudinal Study of the High School Class of 1972 (NLS 72)\n",
      "National Longitudinal Study of the High School Class of 1972 (NLS 72)\n",
      "National Education Longitudinal Study of 1988 (NELS 88)\n",
      "National Longitudinal Survey of the Youth (NLSY)\n",
      "National Longitudinal Study of the High School Class of 1972 (NLS 72)\n",
      "National Longitudinal Study of the High School Class of 1972 (NLS 72)\n",
      "National Longitudinal Study of the High School Class of 1972 (NLS 72)\n",
      "National Education Longitudinal Study of 1988 (NELS 88)\n",
      "National Longitudinal Survey of the Youth (NLSY)\n",
      "National Longitudinal Study of the High School Class of 1972 (NLS 72)\n",
      "National Education Longitudinal Study of 1988 (NELS 88)\n",
      "National Longitudinal Survey of the Youth (NLSY)\n",
      "National Longitudinal Study of the High School Class of 1972 (NLS 72)\n",
      "National Longitudinal Study of the High School Class of 1972 (NLS 72)\n",
      "National Education Longitudinal Study (NELS)\n",
      "National Education Longitudinal Study (NELS)\n"
     ]
    }
   ],
   "source": [
    "#Obtengo datos de Dataset syn ruler.\n",
    "sent = []\n",
    "start = []\n",
    "end = []\n",
    "label = []\n",
    "label_text = []\n",
    "label_long_text = []\n",
    "data_train = []\n",
    "\n",
    "for doc in tqdm(nlp.pipe(train_df_unique['text_all'][:5], batch_size=50)):\n",
    "    abrv_text = []\n",
    "    for ent in doc.ents:\n",
    "        for abrv in doc._.abbreviations:\n",
    "            text = abrv._.long_form.text\n",
    "            if text not in list3 and not abrv.text.islower() and not text.islower():                \n",
    "                #print(abrv.sent, text)\n",
    "                if abrv.sent == ent.sent:\n",
    "                    print(abrv._.long_form.text + \" (\" + abrv.text + \")\")\n",
    "                    #print(ent.sent)\n",
    "                    #print(f\"{abrv} \\t ({abrv.start}, {abrv.end}) {abrv._.long_form}\")                    \n",
    "                    number = abrv.start_char - abrv._.long_form.end_char\n",
    "                    if 0 <= number <= 5:\n",
    "                        #print(abrv.sent)\n",
    "                        #print(f\"{abrv} \\t ({abrv.start}, {abrv.end}) {abrv._.long_form}\")\n",
    "                        #print(abrv.start_char, abrv._.long_form.end_char)\n",
    "                        start_ = abrv._.long_form.start_char - abrv.sent.start_char\n",
    "                        end_ = (abrv.end_char + 1) - abrv.sent.start_char\n",
    "                        #print(abrv.sent.start_char, start_, end_, abrv.end_char - abrv.sent.start_char, abrv.sent.end_char)\n",
    "                        sentence = abrv.sent\n",
    "                        ent_text = abrv._.long_form.text + \" (\" + abrv.text + \")\"\n",
    "                        #print(ent_text)\n",
    "                        sent.append(sentence.text)\n",
    "                        start.append(start_)\n",
    "                        end.append(end_)\n",
    "                        label.append(\"DATASET\")\n",
    "                        label_text.append(ent_text)\n",
    "                        label_long_text.append(abrv._.long_form.text)\n",
    "                        data_train.append(\"SI\")\n",
    "                    else:  \n",
    "                        start_ = abrv.start_char - abrv.sent.start_char\n",
    "                        end_ = abrv.end_char - abrv.sent.start_char\n",
    "                        sentence = abrv.sent\n",
    "                        ent_text = abrv.text\n",
    "                        sent.append(sentence.text)\n",
    "                        start.append(start_)\n",
    "                        end.append(end_)\n",
    "                        label.append(\"DATASET\")\n",
    "                        label_text.append(ent_text)\n",
    "                        label_long_text.append(abrv._.long_form.text)\n",
    "                        data_train.append(\"SI\")\n",
    "                else:\n",
    "                    number = abrv.start_char - abrv._.long_form.end_char\n",
    "                    if 0 <= number <= 5:\n",
    "                        #print(abrv.sent)\n",
    "                        #print(f\"{abrv} \\t ({abrv.start}, {abrv.end}) {abrv._.long_form}\")\n",
    "                        #print(abrv.start_char, abrv._.long_form.end_char)\n",
    "                        start_ = abrv._.long_form.start_char - abrv.sent.start_char\n",
    "                        end_ = (abrv.end_char + 1) - abrv.sent.start_char\n",
    "                        #print(abrv.sent.start_char, start_, end_, abrv.end_char - abrv.sent.start_char, abrv.sent.end_char)\n",
    "                        sentence = abrv.sent\n",
    "                        ent_text = abrv._.long_form.text + \" (\" + abrv.text + \")\"\n",
    "                        print(ent_text)\n",
    "                        sent.append(sentence.text)\n",
    "                        start.append(start_)\n",
    "                        end.append(end_)\n",
    "                        label.append(\"DATASET\")\n",
    "                        label_text.append(ent_text)\n",
    "                        label_long_text.append(abrv._.long_form.text)\n",
    "                        data_train.append(\"NO\")\n",
    "                    else:  \n",
    "                        start_ = abrv.start_char - abrv.sent.start_char\n",
    "                        end_ = abrv.end_char - abrv.sent.start_char\n",
    "                        sentence = abrv.sent\n",
    "                        ent_text = abrv.text\n",
    "                        sent.append(sentence.text)\n",
    "                        start.append(start_)\n",
    "                        end.append(end_)\n",
    "                        label.append(\"DATASET\")\n",
    "                        label_text.append(ent_text)\n",
    "                        label_long_text.append(abrv._.long_form.text)\n",
    "                        data_train.append(\"NO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3561da1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "      <th>label_long_text</th>\n",
       "      <th>data_train</th>\n",
       "      <th>ent_freq</th>\n",
       "      <th>sent_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P olicy reform initiatives such as the No Chil...</td>\n",
       "      <td>39</td>\n",
       "      <td>66</td>\n",
       "      <td>DATASET</td>\n",
       "      <td>No Child Left Behind (NCLB)</td>\n",
       "      <td>No Child Left Behind</td>\n",
       "      <td>NO</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Educators need information about school comple...</td>\n",
       "      <td>169</td>\n",
       "      <td>173</td>\n",
       "      <td>DATASET</td>\n",
       "      <td>NCLB</td>\n",
       "      <td>No Child Left Behind</td>\n",
       "      <td>NO</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Using the nationally representative longitudin...</td>\n",
       "      <td>49</td>\n",
       "      <td>104</td>\n",
       "      <td>DATASET</td>\n",
       "      <td>National Education Longitudinal Study of 1988 ...</td>\n",
       "      <td>National Education Longitudinal Study of 1988</td>\n",
       "      <td>SI</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NELS 88 the data set analyzed in this study in...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>DATASET</td>\n",
       "      <td>NELS 88</td>\n",
       "      <td>National Education Longitudinal Study of 1988</td>\n",
       "      <td>NO</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>On the basis of the data from the base year pa...</td>\n",
       "      <td>64</td>\n",
       "      <td>68</td>\n",
       "      <td>DATASET</td>\n",
       "      <td>NELS</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>NO</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>In preparing the NELS data set for estimation ...</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "      <td>DATASET</td>\n",
       "      <td>NELS</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>SI</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Source Authors calculations using the National...</td>\n",
       "      <td>77</td>\n",
       "      <td>81</td>\n",
       "      <td>DATASET</td>\n",
       "      <td>NELS</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>NO</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>We use the National Education Longitudinal Stu...</td>\n",
       "      <td>50</td>\n",
       "      <td>54</td>\n",
       "      <td>DATASET</td>\n",
       "      <td>NELS</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>NO</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>The NELS follows and retests the same students...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>DATASET</td>\n",
       "      <td>NELS</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>NO</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>However our theoretical framework (as well as ...</td>\n",
       "      <td>188</td>\n",
       "      <td>192</td>\n",
       "      <td>DATASET</td>\n",
       "      <td>NELS</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>NO</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  start  end    label  \\\n",
       "0    P olicy reform initiatives such as the No Chil...     39   66  DATASET   \n",
       "1    Educators need information about school comple...    169  173  DATASET   \n",
       "2    Using the nationally representative longitudin...     49  104  DATASET   \n",
       "3    NELS 88 the data set analyzed in this study in...      0    7  DATASET   \n",
       "4    On the basis of the data from the base year pa...     64   68  DATASET   \n",
       "..                                                 ...    ...  ...      ...   \n",
       "108  In preparing the NELS data set for estimation ...     17   21  DATASET   \n",
       "109  Source Authors calculations using the National...     77   81  DATASET   \n",
       "110  We use the National Education Longitudinal Stu...     50   54  DATASET   \n",
       "111  The NELS follows and retests the same students...      4    8  DATASET   \n",
       "112  However our theoretical framework (as well as ...    188  192  DATASET   \n",
       "\n",
       "                                            label_text  \\\n",
       "0                          No Child Left Behind (NCLB)   \n",
       "1                                                 NCLB   \n",
       "2    National Education Longitudinal Study of 1988 ...   \n",
       "3                                              NELS 88   \n",
       "4                                                 NELS   \n",
       "..                                                 ...   \n",
       "108                                               NELS   \n",
       "109                                               NELS   \n",
       "110                                               NELS   \n",
       "111                                               NELS   \n",
       "112                                               NELS   \n",
       "\n",
       "                                   label_long_text data_train  ent_freq  \\\n",
       "0                             No Child Left Behind         NO         5   \n",
       "1                             No Child Left Behind         NO         5   \n",
       "2    National Education Longitudinal Study of 1988         SI         9   \n",
       "3    National Education Longitudinal Study of 1988         NO         5   \n",
       "4            National Education Longitudinal Study         NO        28   \n",
       "..                                             ...        ...       ...   \n",
       "108          National Education Longitudinal Study         SI        28   \n",
       "109          National Education Longitudinal Study         NO        28   \n",
       "110          National Education Longitudinal Study         NO        28   \n",
       "111          National Education Longitudinal Study         NO        28   \n",
       "112          National Education Longitudinal Study         NO        28   \n",
       "\n",
       "     sent_freq  \n",
       "0            5  \n",
       "1            5  \n",
       "2           10  \n",
       "3           10  \n",
       "4            5  \n",
       "..         ...  \n",
       "108          1  \n",
       "109          1  \n",
       "110          1  \n",
       "111          1  \n",
       "112          1  \n",
       "\n",
       "[113 rows x 9 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df = pd.DataFrame()\n",
    "sentence_df['sentence'] = sent\n",
    "sentence_df['start'] = start\n",
    "sentence_df['end'] = end\n",
    "sentence_df['label'] = label\n",
    "sentence_df['label_text'] = label_text\n",
    "sentence_df['label_long_text'] = label_long_text\n",
    "sentence_df['data_train'] = data_train\n",
    "sentence_df['ent_freq'] = sentence_df.groupby('label_text')['label_text'].transform('count')\n",
    "sentence_df['sent_freq'] = sentence_df.groupby('sentence')['sentence'].transform('count')\n",
    "sentence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "254d9ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df.to_csv(RAW_DATA_DIR + 'dataset_example_to_do_the_analysis_manually.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171131fc",
   "metadata": {},
   "source": [
    "## Remember that one of the most complex processes was the manual cleaning of the sentences, which is the next step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
