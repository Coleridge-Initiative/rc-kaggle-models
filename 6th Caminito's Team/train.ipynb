{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fdb6c31",
   "metadata": {},
   "source": [
    "# Training NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecd7c7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Created output directory: models/NER-LAST-VEC-1100-2\u001b[0m\n",
      "Training pipeline: ['ner']\n",
      "Starting with base model 'en_core_web_sm'\n",
      "Replacing component from base model 'ner'\n",
      "Loading vector from model 'models/gensim_vectors'\n",
      "Counting training words (limit=0)\n",
      "\n",
      "Itn  NER Loss   NER P   NER R   NER F   Token %  CPU WPS\n",
      "---  ---------  ------  ------  ------  -------  -------\n",
      " 25%|████████▉                           | 7625/30805 [00:01<00:03, 6206.86it/s]^C\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "!python -m spacy train en models/NER-LAST-VEC-1100 'data/processed/train_last_sent_929_format.json' 'data/processed/dev_last_sent_164_format.json' --base-model 'en_core_web_sm' --vectors 'models/gensim_vectors'  -p ner -R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814b9632",
   "metadata": {},
   "source": [
    "### Adding Entity Ruler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca248bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asthetics https://www.kaggle.com/manabendrarout/tabular-data-preparation-basic-eda-and-baseline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Basic\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from tqdm.autonotebook import tqdm\n",
    "import string\n",
    "import re\n",
    "from functools import partial\n",
    "from ipywidgets import IntProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd96443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload Kaggle Data\n",
    "train_df = pd.read_csv('../../coleridgeinitiative-show-us-the-data/train.csv')\n",
    "sample_sub = pd.read_csv('../../coleridgeinitiative-show-us-the-data/sample_submission.csv')\n",
    "train_files_path = '../../coleridgeinitiative-show-us-the-data/train'\n",
    "test_files_path = '../../coleridgeinitiative-show-us-the-data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd11e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('SETTINGS.json',)\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "\n",
    "RAW_DATA_DIR = data['RAW_DATA_DIR']\n",
    "TRAIN_DATA_CLEAN_PATH = data['TRAIN_DATA_CLEAN_PATH']\n",
    "TEST_DATA_CLEAN_PATH = data['TEST_DATA_CLEAN_PATH']\n",
    "MODEL_CHECKPOINT_DIR = data['MODEL_CHECKPOINT_DIR']\n",
    "SUBMISSION_DIR = data['SUBMISSION_DIR']\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65efd3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    '''\n",
    "    Removes special charecters, multiple spaces\n",
    "    text - Sentence that needs to be cleaned\n",
    "    '''\n",
    "    text = re.sub(r'[^A-Za-z0-9.!?'\"'\"'()\\[\\]]+', ' ', text)\n",
    "    text = re.sub(\"'\", '', text)\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = re.sub('[.]{2,}', '.', text)\n",
    "    text = re.sub(r'\\. \\.', '.', text)\n",
    "    text = re.sub(r' \\.', '.', text)    \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "980639c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_append_return(filename, train_files_path=train_files_path, output='text'):\n",
    "    json_path = os.path.join(train_files_path, (filename+'.json'))\n",
    "    headings = []\n",
    "    contents = []\n",
    "    combined = []\n",
    "    with open(json_path, 'r') as f: #encoding='utf-8'\n",
    "        json_decode = json.load(f)\n",
    "        for data in json_decode:\n",
    "            headings.append(data.get('section_title'))\n",
    "            s = text_cleaning(data.get('text'))\n",
    "            if len(s) > 200000:\n",
    "                #s = data.get('text')\n",
    "                #print(data.get('text'))\n",
    "                l = s.split()\n",
    "                n = 100000\n",
    "                texto = [\" \".join(l[x:x+n]) for x in range(0, len(l), n)]   \n",
    "                contents.extend(texto)\n",
    "            else:\n",
    "                contents.append(s)\n",
    "                #print(contents1)\n",
    "            combined.append(data.get('section_title'))\n",
    "            combined.append(data.get('text'))\n",
    "    \n",
    "    all_headings = ' '.join(headings)\n",
    "    all_contents = ' '.join(contents)\n",
    "    #contents = contents1.extend(contents2)\n",
    "    #print(combined)\n",
    "    all_data = '. '.join(combined)\n",
    "    \n",
    "    if output == 'text':\n",
    "        return all_contents\n",
    "    elif output == 'head':\n",
    "        return all_headings\n",
    "    elif output == 'comb':\n",
    "        return contents\n",
    "    else:\n",
    "        return all_data     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "707a823c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca760982059d4df8bcb92067e10c7c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19661 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "train_df['text_all'] = train_df['Id'].progress_apply(partial(read_append_return, output='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc52536a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7d3c21b61245c3a4551490a89c6814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19661 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "train_df['text_all'] = train_df['text_all'].progress_apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c82234db",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_vec = pd.read_csv(RAW_DATA_DIR + 'cleanedLabel_toVec_Diego_for_vectors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7f12b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1039 1189\n"
     ]
    }
   ],
   "source": [
    "#Sin minusculas\n",
    "temp_1 = [text_cleaning(x).lower().rstrip() for x in train_df['dataset_label'].unique()]\n",
    "temp_2 = [text_cleaning(x).lower().rstrip() for x in train_df['dataset_title'].unique()]\n",
    "temp_3 = [text_cleaning(x).lower().rstrip() for x in train_df['cleaned_label'].unique()]\n",
    "temp_4 = [text_cleaning(x).lower().rstrip() for x in to_vec['0'].unique()]\n",
    "#temp_4 = [text_cleaning(x).lower().rstrip() for x in to_vec['title'].unique()]\n",
    "\n",
    "\n",
    "existing_labels = set(temp_1 + temp_2 + temp_3 + temp_4)\n",
    "\n",
    "#Minusculas y si espacio al final, para LOWER patterns\n",
    "temp_1_low = [text_cleaning(x).rstrip() for x in train_df['dataset_label'].unique()]\n",
    "temp_2_low = [text_cleaning(x).rstrip() for x in train_df['dataset_title'].unique()]\n",
    "temp_3_low = [text_cleaning(x).rstrip() for x in to_vec['0'].unique()]\n",
    "\n",
    "existing_labels_text = set(temp_1_low + temp_2_low + temp_3_low)\n",
    "\n",
    "print(len(existing_labels), len(existing_labels_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dfd0375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(MODEL_CHECKPOINT_DIR + 'NER-LAST-VEC-1100/model-best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4eb9438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1043"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#genero patterns para spacy Lower y text para len 1\n",
    "patterns = []\n",
    "\n",
    "\n",
    "for dataset in existing_labels:\n",
    "    len_data = dataset.split()\n",
    "    if len(len_data) > 2:\n",
    "        #print(dataset)\n",
    "        phrase = []\n",
    "        for word in nlp(dataset):\n",
    "            pattern = {}\n",
    "            pattern[\"LOWER\"] = str(word)\n",
    "            phrase.append(pattern)\n",
    "        #patterns.append({\"label\": dataset, \"pattern\": phrase})\n",
    "        patterns.append({\"label\": \"RULDATA\", \"pattern\": phrase})\n",
    "\n",
    "\n",
    "for dataset in existing_labels_text:\n",
    "    len_data = dataset.split()\n",
    "    if len(len_data) < 3:\n",
    "        #print(dataset)\n",
    "        phrase = []\n",
    "        for word in nlp(dataset):\n",
    "            pattern = {}\n",
    "            pattern[\"TEXT\"] = str(word)\n",
    "            phrase.append(pattern)\n",
    "        #patterns.append({\"label\": dataset, \"pattern\": phrase})\n",
    "        patterns.append({\"label\": \"RULDATA\", \"pattern\": phrase})\n",
    "\n",
    "\n",
    "len(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22049830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'entity_ruler']\n"
     ]
    }
   ],
   "source": [
    "from spacy.pipeline import EntityRuler\n",
    "ruler = EntityRuler(nlp)\n",
    "nlp.add_pipe(ruler, after='ner')\n",
    "ruler.add_patterns(patterns)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceacebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving NLP model\n",
    "#nlp.to_disk(MODEL_CHECKPOINT_DIR +\"NER-LAST-VEC-1100-RULER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf742a45",
   "metadata": {},
   "source": [
    "# Training TEXTCAT Dataset recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2155a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is modified from spaCy's user guide for TextCategorizer training \n",
    "from __future__ import unicode_literals, print_function\n",
    "from __future__ import unicode_literals\n",
    "import copy\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "\n",
    "import warnings \n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import re\n",
    "#Lo saco de otro codigo para limpiar un poco\n",
    "def clean_string(mystring):\n",
    "    return re.sub('[^A-Za-z\\ 0-9 ]+', '', mystring)\n",
    "\n",
    "def main(model=None, n_iter=5, init_tok2vec=None):\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"es\")  # create blank Language class\n",
    "        print(\"Created blank 'es' model\")\n",
    "\n",
    "    # add the text classifier to the pipeline if it doesn't exist\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"textcat\" not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe(\n",
    "            \"textcat\",\n",
    "            config={\n",
    "                \"exclusive_classes\": True,\n",
    "                \"architecture\": \"ensemble\",\n",
    "            }\n",
    "        )\n",
    "        nlp.add_pipe(textcat, last=True)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        textcat = nlp.get_pipe(\"textcat\")\n",
    "\n",
    "    for i in ['SI','NO']:\n",
    "        textcat.add_label(i)\n",
    "    \n",
    "    # load the datasets\n",
    "    print(\"Loading data...\")\n",
    "    #Train\n",
    "    df = pd.read_csv(TRAIN_DATA_CLEAN_PATH + '3037_for_textcat_train.csv')\n",
    "    #df.drop(['text'], axis=1, inplace=True)\n",
    "    df = df[df['TextCat'] != 'empty']\n",
    "\n",
    "    conclusion_values = df['TextCat'].unique()\n",
    "    labels_default = dict((v, 0) for v in conclusion_values)\n",
    "\n",
    "    train_data = []\n",
    "    for i, row in df.iterrows():\n",
    "\n",
    "        label_values = copy.deepcopy(labels_default)\n",
    "        label_values[row['TextCat']] = 1\n",
    "\n",
    "        train_data.append((str(row['sentence']), {\"cats\": label_values}))\n",
    "\n",
    "    train_data = train_data[:5000]    \n",
    "\n",
    "    \n",
    "    #dev\n",
    "    df_dev = pd.read_csv(TEST_DATA_CLEAN_PATH + '759_for_textcat_dev.csv')\n",
    "    #df_dev.drop(['text'], axis=1, inplace=True)\n",
    "    df_dev = df_dev[df_dev['TextCat'] != 'empty']\n",
    "\n",
    "    conclusion_dev_values = df_dev['TextCat'].unique()\n",
    "    labels_dev_default = dict((v, 0) for v in conclusion_dev_values)\n",
    "\n",
    "    dev_data = []\n",
    "    for i, row in df_dev.iterrows():\n",
    "\n",
    "        label_dev_values = copy.deepcopy(labels_dev_default)\n",
    "        label_dev_values[row['TextCat']] = 1\n",
    "\n",
    "        dev_data.append((str(row['sentence']), {\"cats\": label_dev_values}))\n",
    "\n",
    "    dev_data = dev_data\n",
    "    \n",
    "    print(\n",
    "        \"Using {} examples ({} training, {} evaluation)\".format(\n",
    "            len(df['sentence']) + len(df_dev['sentence']), len(df['sentence']), len(df_dev['sentence'])\n",
    "        )\n",
    "    )\n",
    "   \n",
    "    #test\n",
    "    df_test = pd.read_csv(TEST_DATA_CLEAN_PATH + '759_for_textcat_dev.csv')\n",
    "    #df_test.drop(['text'], axis=1, inplace=True)\n",
    "    df_test = df_test[df_test['TextCat'] != 'empty']\n",
    "\n",
    "    conclusion_test_values = df_test['TextCat'].unique()\n",
    "    labels_test_default = dict((v, 0) for v in conclusion_test_values)\n",
    "\n",
    "    test_data = []\n",
    "    for i, row in df_test.iterrows():\n",
    "\n",
    "        label_test_values = copy.deepcopy(labels_test_default)\n",
    "        label_test_values[row['TextCat']] = 1\n",
    "\n",
    "        test_data.append((str(clean_string(row['sentence'])), {\"cats\": label_test_values}))\n",
    "\n",
    "    test_data = test_data\n",
    "    \n",
    "    #Aca hago el cambio\n",
    "    train_ls = train_data\n",
    "    valid_ls = dev_data\n",
    "    test_ls = test_data\n",
    "    \n",
    "    # Convert valid text and label to list.\n",
    "    valid_text, valid_label = list(zip(*valid_ls))\n",
    "\n",
    "    # Convert test text and label to list.\n",
    "    test_text, test_label = list(zip(*test_ls))\n",
    "    \n",
    "    n_iter = 20\n",
    "    print_every= 1\n",
    "    not_improve = 5 \n",
    "\n",
    "\n",
    "# Train model\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "        optimizer = nlp.begin_training() # initiate a new model with random weights\n",
    "        if init_tok2vec is not None:\n",
    "            init_tok2vec = Path(init_tok2vec)\n",
    "            print(\"Loaded Vector model '%s'\" % init_tok2vec)\n",
    "            with init_tok2vec.open(\"rb\") as file_:\n",
    "                textcat.model.tok2vec.from_bytes(file_.read())                \n",
    "        print(\"Training the model...\")\n",
    "\n",
    "        score_f1_best = 0\n",
    "        early_stop = 0\n",
    "\n",
    "        for i in range(n_iter):\n",
    "            losses = {}\n",
    "            true_labels = list() # true label\n",
    "            pdt_labels = list() # predict label\n",
    "\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            random.shuffle(train_ls)  # shuffle training data every iteration\n",
    "            batches = minibatch(train_ls, size=compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n",
    "                           losses=losses)\n",
    "\n",
    "            with textcat.model.use_params(optimizer.averages): \n",
    "                # evaluate on valid_text, valid_label\n",
    "                docs = [nlp.tokenizer(text) for text in valid_text]\n",
    "\n",
    "                for j, doc in enumerate(textcat.pipe(docs)):\n",
    "                    true_series = pd.Series(valid_label[j]['cats'])\n",
    "                    true_label = true_series.idxmax()  # idxmax() is the new version of argmax() \n",
    "                    true_labels.append(true_label)\n",
    "\n",
    "                    pdt_series = pd.Series(doc.cats)\n",
    "                    pdt_label = pdt_series.idxmax()  # idxmax() is the new version of argmax() \n",
    "                    pdt_labels.append(pdt_label)\n",
    "\n",
    "                score_f1 = f1_score(true_labels, pdt_labels, average='weighted')\n",
    "                score_ac = accuracy_score(true_labels, pdt_labels)\n",
    "\n",
    "                if i % print_every == 0:\n",
    "                    print('textcat loss: {:.4f}\\tf1 score: {:.3f}\\taccuracy: {:.3f}'.format(\n",
    "                        losses['textcat'],score_f1, score_ac))\n",
    "\n",
    "                if score_f1 > score_f1_best:\n",
    "                    early_stop = 0\n",
    "                    score_f1_best = score_f1\n",
    "                    with nlp.use_params(optimizer.averages):\n",
    "                        output_dir = Path(MODEL_CHECKPOINT_DIR + 'Ensemble_3000_dataset_vs_ORG')\n",
    "                        if not output_dir.exists():\n",
    "                              output_dir.mkdir()\n",
    "                        nlp.to_disk(output_dir) # save the model\n",
    "                else:\n",
    "                    early_stop += 1\n",
    "\n",
    "                if early_stop >= not_improve:\n",
    "                    print('Finished training...')\n",
    "                    break\n",
    "\n",
    "                if i == n_iter:\n",
    "                    print('Finished training...')\n",
    "                #return {\"textcat_a\": score_ac, \"textcat_l\": losses['textcat'], \"textcat_f\": score_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98f760d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'es' model\n",
      "Loading data...\n",
      "Using 3796 examples (3037 training, 759 evaluation)\n",
      "Training the model...\n",
      "textcat loss: 8.0106\tf1 score: 0.921\taccuracy: 0.922\n",
      "textcat loss: 3.4629\tf1 score: 0.926\taccuracy: 0.926\n",
      "textcat loss: 2.4774\tf1 score: 0.926\taccuracy: 0.926\n",
      "textcat loss: 1.8975\tf1 score: 0.929\taccuracy: 0.929\n",
      "textcat loss: 1.6813\tf1 score: 0.930\taccuracy: 0.930\n",
      "textcat loss: 1.5195\tf1 score: 0.934\taccuracy: 0.934\n",
      "textcat loss: 1.1593\tf1 score: 0.929\taccuracy: 0.929\n",
      "textcat loss: 1.1381\tf1 score: 0.927\taccuracy: 0.928\n",
      "textcat loss: 0.9552\tf1 score: 0.928\taccuracy: 0.929\n",
      "textcat loss: 0.7078\tf1 score: 0.923\taccuracy: 0.924\n",
      "textcat loss: 1.1103\tf1 score: 0.930\taccuracy: 0.930\n",
      "Finished training...\n"
     ]
    }
   ],
   "source": [
    "#train_ensemble_model \n",
    "bb = main(model=None, init_tok2vec=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dda2c52",
   "metadata": {},
   "source": [
    "# Training TEXTCAT Sentence recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd564480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is modified from spaCy's user guide for TextCategorizer training \n",
    "from __future__ import unicode_literals, print_function\n",
    "from __future__ import unicode_literals\n",
    "import copy\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "\n",
    "import warnings \n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import re\n",
    "#Lo saco de otro codigo para limpiar un poco\n",
    "def clean_string(mystring):\n",
    "    return re.sub('[^A-Za-z\\ 0-9 ]+', '', mystring)\n",
    "\n",
    "def main(model=None, n_iter=5, init_tok2vec=None):\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"es\")  # create blank Language class\n",
    "        print(\"Created blank 'es' model\")\n",
    "\n",
    "    # add the text classifier to the pipeline if it doesn't exist\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"textcat\" not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe(\n",
    "            \"textcat\",\n",
    "            config={\n",
    "                \"exclusive_classes\": True,\n",
    "                \"architecture\": \"ensemble\",\n",
    "            }\n",
    "        )\n",
    "        nlp.add_pipe(textcat, last=True)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        textcat = nlp.get_pipe(\"textcat\")\n",
    "\n",
    "    for i in ['SI','NO']:\n",
    "        textcat.add_label(i)\n",
    "    \n",
    "    # load the datasets\n",
    "    print(\"Loading data...\")\n",
    "    #Train\n",
    "    df = pd.read_csv(TRAIN_DATA_CLEAN_PATH + '30000_for_textcat_train.csv')\n",
    "    #df.drop(['text'], axis=1, inplace=True)\n",
    "    df = df[df['TextCat'] != 'empty']\n",
    "\n",
    "    conclusion_values = df['TextCat'].unique()\n",
    "    labels_default = dict((v, 0) for v in conclusion_values)\n",
    "\n",
    "    train_data = []\n",
    "    for i, row in df.iterrows():\n",
    "\n",
    "        label_values = copy.deepcopy(labels_default)\n",
    "        label_values[row['TextCat']] = 1\n",
    "\n",
    "        train_data.append((str(row['sentence']), {\"cats\": label_values}))\n",
    "\n",
    "    train_data = train_data[:5000]    \n",
    "\n",
    "    \n",
    "    #dev\n",
    "    df_dev = pd.read_csv(TEST_DATA_CLEAN_PATH + '30000_for_textcat_dev.csv')\n",
    "    #df_dev.drop(['text'], axis=1, inplace=True)\n",
    "    df_dev = df_dev[df_dev['TextCat'] != 'empty']\n",
    "\n",
    "    conclusion_dev_values = df_dev['TextCat'].unique()\n",
    "    labels_dev_default = dict((v, 0) for v in conclusion_dev_values)\n",
    "\n",
    "    dev_data = []\n",
    "    for i, row in df_dev.iterrows():\n",
    "\n",
    "        label_dev_values = copy.deepcopy(labels_dev_default)\n",
    "        label_dev_values[row['TextCat']] = 1\n",
    "\n",
    "        dev_data.append((str(row['sentence']), {\"cats\": label_dev_values}))\n",
    "\n",
    "    dev_data = dev_data\n",
    "    \n",
    "    print(\n",
    "        \"Using {} examples ({} training, {} evaluation)\".format(\n",
    "            len(df['sentence']) + len(df_dev['sentence']), len(df['sentence']), len(df_dev['sentence'])\n",
    "        )\n",
    "    )\n",
    "   \n",
    "    #test\n",
    "    df_test = pd.read_csv(TEST_DATA_CLEAN_PATH + '30000_for_textcat_dev.csv')\n",
    "    #df_test.drop(['text'], axis=1, inplace=True)\n",
    "    df_test = df_test[df_test['TextCat'] != 'empty']\n",
    "\n",
    "    conclusion_test_values = df_test['TextCat'].unique()\n",
    "    labels_test_default = dict((v, 0) for v in conclusion_test_values)\n",
    "\n",
    "    test_data = []\n",
    "    for i, row in df_test.iterrows():\n",
    "\n",
    "        label_test_values = copy.deepcopy(labels_test_default)\n",
    "        label_test_values[row['TextCat']] = 1\n",
    "\n",
    "        test_data.append((str(clean_string(row['sentence'])), {\"cats\": label_test_values}))\n",
    "\n",
    "    test_data = test_data\n",
    "    \n",
    "    #Aca hago el cambio\n",
    "    train_ls = train_data\n",
    "    valid_ls = dev_data\n",
    "    test_ls = test_data\n",
    "    \n",
    "    # Convert valid text and label to list.\n",
    "    valid_text, valid_label = list(zip(*valid_ls))\n",
    "\n",
    "    # Convert test text and label to list.\n",
    "    test_text, test_label = list(zip(*test_ls))\n",
    "    \n",
    "    n_iter = 20\n",
    "    print_every= 1\n",
    "    not_improve = 5 \n",
    "\n",
    "\n",
    "# Train model\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "        optimizer = nlp.begin_training() # initiate a new model with random weights\n",
    "        if init_tok2vec is not None:\n",
    "            init_tok2vec = Path(init_tok2vec)\n",
    "            print(\"Loaded Vector model '%s'\" % init_tok2vec)\n",
    "            with init_tok2vec.open(\"rb\") as file_:\n",
    "                textcat.model.tok2vec.from_bytes(file_.read())                \n",
    "        print(\"Training the model...\")\n",
    "\n",
    "        score_f1_best = 0\n",
    "        early_stop = 0\n",
    "\n",
    "        for i in range(n_iter):\n",
    "            losses = {}\n",
    "            true_labels = list() # true label\n",
    "            pdt_labels = list() # predict label\n",
    "\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            random.shuffle(train_ls)  # shuffle training data every iteration\n",
    "            batches = minibatch(train_ls, size=compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n",
    "                           losses=losses)\n",
    "\n",
    "            with textcat.model.use_params(optimizer.averages): \n",
    "                # evaluate on valid_text, valid_label\n",
    "                docs = [nlp.tokenizer(text) for text in valid_text]\n",
    "\n",
    "                for j, doc in enumerate(textcat.pipe(docs)):\n",
    "                    true_series = pd.Series(valid_label[j]['cats'])\n",
    "                    true_label = true_series.idxmax()  # idxmax() is the new version of argmax() \n",
    "                    true_labels.append(true_label)\n",
    "\n",
    "                    pdt_series = pd.Series(doc.cats)\n",
    "                    pdt_label = pdt_series.idxmax()  # idxmax() is the new version of argmax() \n",
    "                    pdt_labels.append(pdt_label)\n",
    "\n",
    "                score_f1 = f1_score(true_labels, pdt_labels, average='weighted')\n",
    "                score_ac = accuracy_score(true_labels, pdt_labels)\n",
    "\n",
    "                if i % print_every == 0:\n",
    "                    print('textcat loss: {:.4f}\\tf1 score: {:.3f}\\taccuracy: {:.3f}'.format(\n",
    "                        losses['textcat'],score_f1, score_ac))\n",
    "\n",
    "                if score_f1 > score_f1_best:\n",
    "                    early_stop = 0\n",
    "                    score_f1_best = score_f1\n",
    "                    with nlp.use_params(optimizer.averages):\n",
    "                        output_dir = Path(MODEL_CHECKPOINT_DIR + 'Ensemble_30000_textcat')\n",
    "                        if not output_dir.exists():\n",
    "                              output_dir.mkdir()\n",
    "                        nlp.to_disk(output_dir) # save the model\n",
    "                else:\n",
    "                    early_stop += 1\n",
    "\n",
    "                if early_stop >= not_improve:\n",
    "                    print('Finished training...')\n",
    "                    break\n",
    "\n",
    "                if i == n_iter:\n",
    "                    print('Finished training...')\n",
    "                #return {\"textcat_a\": score_ac, \"textcat_l\": losses['textcat'], \"textcat_f\": score_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4c7741",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_ensemble_model \n",
    "bb = main(model=None, init_tok2vec=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
